import os
import sys
import threading
import time
import webbrowser
import logging
import io
import tkinter as tk
from tkinter import filedialog, messagebox
from flask import Flask, request, jsonify, send_file, send_from_directory, make_response, render_template_string
from werkzeug.utils import secure_filename
from flask_cors import CORS
from glob import iglob
import dataclasses
import struct
import csv
import tempfile
import shutil
from datetime import datetime, timezone
import socket
import atexit
import signal
import uuid
import concurrent.futures
import subprocess

# ========================================================
# ğŸ›¡ï¸ è¿è¡Œæ—¶ç¯å¢ƒ
# ========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
candidates = [current_dir, os.path.join(current_dir, "epycon")]
for path in candidates:
    if os.path.exists(path) and path not in sys.path:
        sys.path.insert(0, path)

def resource_path(relative_path):
    """ è·å–èµ„æºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå…¼å®¹å¼€å‘ç¯å¢ƒä¸ PyInstaller æ‰“åŒ…ç¯å¢ƒ """
    try:
        base_path = sys._MEIPASS  # type: ignore
    except Exception:
        # æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½• current_dirï¼Œè€Œä¸æ˜¯è¿è¡Œæ—¶çš„ CWD
        base_path = current_dir
    return os.path.join(base_path, relative_path)

# ========================================================
# ğŸ”’ å•å®ä¾‹æ£€æŸ¥å’Œç«¯å£ç®¡ç†
# ========================================================
LOCK_FILE = None

def check_single_instance():
    """æ£€æŸ¥æ˜¯å¦å·²æœ‰å®ä¾‹åœ¨è¿è¡Œ"""
    global LOCK_FILE
    lock_path = os.path.join(tempfile.gettempdir(), 'epycon_gui.lock')
    current_pid = os.getpid()
    is_subprocess = os.environ.get('WERKZEUG_RUN_MAIN') == 'true'
    
    try:
        # å°è¯•åˆ›å»ºé”æ–‡ä»¶
        if os.path.exists(lock_path):
            # æ£€æŸ¥é”æ–‡ä»¶ä¸­çš„ PID æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            try:
                with open(lock_path, 'r') as f:
                    lock_data = f.read().strip().split(',')
                    old_pid = int(lock_data[0])
                    lock_parent_pid = int(lock_data[1]) if len(lock_data) > 1 else None
                
                # å¦‚æœå½“å‰è¿›ç¨‹æ˜¯ Reloader çš„å­è¿›ç¨‹ï¼Œä¸”çˆ¶è¿›ç¨‹ PID ç›¸åŒï¼Œåˆ™å…è®¸
                if is_subprocess and lock_parent_pid is not None:
                    parent_pid = os.getppid() if hasattr(os, 'getppid') else None
                    if parent_pid == lock_parent_pid:
                        # è¿™æ˜¯åŒä¸€ä¸ª Reloader å¯åŠ¨çš„å­è¿›ç¨‹ï¼Œå…è®¸ç»§ç»­
                        return True
                
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
                if os.name == 'nt':
                    try:
                        import psutil
                        # æ£€æŸ¥ old_pid å’Œé”æ–‡ä»¶ä¸­çš„çˆ¶è¿›ç¨‹æ˜¯å¦éƒ½è¿˜æ´»ç€
                        if psutil.pid_exists(old_pid):
                            print(f"âš ï¸ æ£€æµ‹åˆ°å¦ä¸€ä¸ªå®ä¾‹æ­£åœ¨è¿è¡Œ (PID: {old_pid})")
                            print("è¯·å…ˆå…³é—­å…¶ä»–å®ä¾‹ï¼Œæˆ–ç­‰å¾…å‡ ç§’åé‡è¯•ã€‚")
                            return False
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰ psutilï¼Œä½¿ç”¨ç®€å•çš„æ—¶é—´æ£€æŸ¥
                        file_age = time.time() - os.path.getmtime(lock_path)
                        if file_age < 60:  # å¦‚æœé”æ–‡ä»¶åœ¨ 1 åˆ†é’Ÿå†…åˆ›å»ºï¼Œè®¤ä¸ºè¿˜åœ¨ä½¿ç”¨
                            # ä½†å¦‚æœæˆ‘ä»¬æ˜¯å­è¿›ç¨‹ä¸”çˆ¶ PID ç›¸åŒï¼Œåˆ™å…è®¸
                            if not (is_subprocess and lock_parent_pid is not None):
                                print(f"âš ï¸ æ£€æµ‹åˆ°é”æ–‡ä»¶ (åˆ›å»ºäº {int(file_age)} ç§’å‰)")
                                print("å¦‚æœç¡®è®¤æ²¡æœ‰å…¶ä»–å®ä¾‹è¿è¡Œï¼Œè¯·æ‰‹åŠ¨åˆ é™¤é”æ–‡ä»¶ï¼š")
                                print(f"   {lock_path}")
                                return False
            except (ValueError, IOError):
                pass
            
            # å¦‚æœè¿›ç¨‹ä¸å­˜åœ¨ï¼Œåˆ é™¤æ—§é”æ–‡ä»¶
            try:
                os.remove(lock_path)
            except Exception:
                pass
        
        # å¦‚æœè¿™æ˜¯ Reloader çš„å­è¿›ç¨‹ï¼Œä¸è¦é‡æ–°åˆ›å»ºé”æ–‡ä»¶
        if is_subprocess:
            return True
        
        # åˆ›å»ºæ–°é”æ–‡ä»¶ï¼ˆè®°å½•çˆ¶è¿›ç¨‹ PID ç”¨äº Reloader è¯†åˆ«ï¼‰
        parent_pid = os.getpid()  # åœ¨ä¸»è¿›ç¨‹ä¸­ï¼Œè‡ªå·±å°±æ˜¯"çˆ¶"
        LOCK_FILE = open(lock_path, 'w')
        LOCK_FILE.write(f"{parent_pid},{parent_pid}")  # æ ¼å¼: current_pid, parent_pid
        LOCK_FILE.flush()
        
        # Windows ä¸Šå°è¯•åŠ é”
        if os.name == 'nt':
            import msvcrt
            try:
                msvcrt.locking(LOCK_FILE.fileno(), msvcrt.LK_NBLCK, 1)
            except Exception:
                pass
        
        return True
    except Exception as e:
        print(f"å•å®ä¾‹æ£€æŸ¥å¤±è´¥: {e}")
        return True  # å‡ºé”™æ—¶å…è®¸ç»§ç»­è¿è¡Œ

def check_port_available(port=5000):
    """æ£€æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        sock.bind(('127.0.0.1', port))
        sock.close()
        return True
    except OSError:
        return False

def kill_port_occupier(port=5000):
    """
    å°è¯•ç»ˆæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹ã€‚
    è¿”å›: (bool, str) -> (æ˜¯å¦æˆåŠŸ/è·³è¿‡, å ç”¨è€…åç§°)
    """
    import subprocess
    try:
        # è·å–å½“å‰è¿›ç¨‹ PID å’Œçˆ¶è¿›ç¨‹ PID
        my_pid = os.getpid()
        ppid = os.getppid()
        
        if os.name == 'nt':
            # Windows é€»è¾‘
            result = subprocess.run(['netstat', '-ano'], capture_output=True, text=True, timeout=5)
            for line in result.stdout.split('\n'):
                if f':{port}' in line and 'LISTENING' in line:
                    parts = line.split()
                    if parts:
                        pid_str = parts[-1]
                        try:
                            target_pid = int(pid_str)
                            # ç¦æ­¢è‡ªæ€æˆ–æ€çˆ¶ï¼ˆReloader ç¯å¢ƒä¸‹å¸¸è§ï¼‰
                            if target_pid == my_pid or target_pid == ppid:
                                return False, "Self/Parent"
                                
                            pname = "Unknown Windows Process"
                            print(f"å‘ç°å ç”¨è€…: {pname} (PID: {target_pid})")
                            subprocess.run(['taskkill', '/F', '/PID', str(target_pid)], timeout=5)
                            time.sleep(1.5)
                            return True, pname
                        except Exception: pass
        else:
            # macOS / Linux é€»è¾‘ (ä½¿ç”¨ lsof)
            try:
                cmd = ['lsof', '-i', f':{port}', '-sTCP:LISTEN', '-n', '-P']
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)
                lines = [l for l in result.stdout.split('\n') if l.strip()]
                if len(lines) > 1:
                    parts = lines[1].split()
                    pname = parts[0]
                    pid_str = parts[1]
                    try:
                        target_pid = int(pid_str)
                        if target_pid == my_pid or target_pid == ppid:
                            # ç«¯å£æ˜¯è¢«è‡ªå·±æˆ–çˆ¶è¿›ç¨‹å ç”¨çš„ï¼ˆä¾‹å¦‚ Flask Reloader å¯åŠ¨ä¸­ï¼‰ï¼Œè·³è¿‡æ¸…ç†
                            return False, "Self/Parent"
                            
                        system_services = ['ControlCe', 'ControlCenter', 'launchd', 'rapportd']
                        if pname in system_services:
                            print(f"ğŸ·ï¸  ç«¯å£ {port} è¢«ç³»ç»ŸæœåŠ¡ '{pname}' å ç”¨ï¼Œå°†å°è¯•è§„é¿ã€‚")
                            return False, pname
                        
                        print(f"å‘ç°å ç”¨è€…: {pname} (PID: {target_pid})")
                        subprocess.run(['kill', '-9', str(target_pid)], timeout=5)
                        time.sleep(1.5)
                        return True, pname
                    except Exception: pass
            except FileNotFoundError:
                print("âš ï¸  ç³»ç»Ÿç¼ºå°‘ 'lsof' æŒ‡ä»¤ã€‚")
    except Exception as e:
        print(f"æ¸…ç†ç«¯å£å¤±è´¥: {e}")
    return False, "None"

def cleanup_on_exit():
    """ç¨‹åºé€€å‡ºæ—¶çš„æ¸…ç†å·¥ä½œ"""
    global LOCK_FILE
    if LOCK_FILE:
        try:
            LOCK_FILE.close()
            lock_path = os.path.join(tempfile.gettempdir(), 'epycon_gui.lock')
            if os.path.exists(lock_path):
                os.remove(lock_path)
        except Exception:
            pass

# æ³¨å†Œé€€å‡ºæ¸…ç†
atexit.register(cleanup_on_exit)

# ========================================================
# ğŸ”§ å¼ºåˆ¶ UTF-8 å†™å…¥
# ========================================================
try:
    import builtins
    _real_open = builtins.open
    class UTF8EnforcedOpen:
        def __enter__(self):
            self.original_open = builtins.open
            def new_open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):
                if 'w' in mode or 'a' in mode:
                    encoding = 'utf-8'
                    errors = 'replace' 
                return self.original_open(file, mode, buffering, encoding, errors, newline, closefd, opener)
            builtins.open = new_open
        def __exit__(self, exc_type, exc_val, exc_tb):
            builtins.open = self.original_open
except ImportError: pass

# ========================================================
# ğŸ“¦ å¯¼å…¥ Epycon
# ========================================================
try:
    from epycon.config.byteschema import ENTRIES_FILENAME, LOG_PATTERN, MASTER_FILENAME
    from epycon.iou import LogParser, EntryPlanter, CSVPlanter, HDFPlanter, readentries, mount_channels
    from epycon.iou.parsers import _readmaster
    from epycon.utils.person import Tokenize
    from epycon.core.helpers import difftimestamp
except ImportError as e:
    print(f"æ— æ³•åŠ è½½ Epyconã€‚\n{e}")
    if __name__ == "__main__":
        sys.exit(1)
    else:
        raise  # åœ¨æµ‹è¯•ç¯å¢ƒä¸­æŠ›å‡ºå¼‚å¸¸ä¾› pytest æ•è·

app = Flask(__name__, static_folder='.', static_url_path='')
CORS(app)

# ========================================================
# ğŸ“ [æ ¸å¿ƒ] å…¨å±€æ—¥å¿—é…ç½® (åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°)
# ========================================================
LOG_PATH = os.path.join(tempfile.gettempdir(), "epycon_gui.log")
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_PATH, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("epycon_web")

class MemoryLogHandler(logging.Handler):
    def __init__(self):
        super().__init__()
        self.logs = []
        self.setLevel(logging.DEBUG)  # æ•è·æ‰€æœ‰çº§åˆ«çš„æ—¥å¿—
    def emit(self, record):
        self.logs.append(self.format(record))

# ========================================================
# ğŸš€ å¼‚æ­¥ä»»åŠ¡ç®¡ç†
# ========================================================
executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
TASKS = {} # taskId -> { 'status': 'running', 'progress': 0, 'logs': [], 'result': None }
# ========================================================
@dataclasses.dataclass
class MutableEntry:
    timestamp: float
    group: str
    message: str
    fid: str = '0'
    duration: float = 0
    color: int = 0

# ========================================================
# âš–ï¸ [æ ¸å¿ƒ] å…¨è‡ªåŠ¨æ—¶é—´å½’ä¸€åŒ– (Unix Seconds)
# ========================================================
def to_unix_seconds(val):
    try:
        if isinstance(val, datetime):
            return val.replace(tzinfo=timezone.utc).timestamp()
        num = float(val)
        if num > 100_000_000_000_000_000: # FILETIME
            return (num - 116444736000000000) / 10_000_000.0
        if num > 100_000_000_000: # Milliseconds
            return num / 1000.0
        return num
    except Exception:
        return 0.0

# ========================================================
# ğŸ› ï¸ [æ ¸å¿ƒ] entries.log å»å£³
# ========================================================
def prepare_standard_entries_file(original_path):
    try:
        with open(original_path, 'rb') as f:
            raw = f.read(256)
        valid_gids = [1, 2, 3, 4, 5, 6, 17]
        target_offset = 0
        gid_128 = struct.unpack_from('<H', raw, 128)[0]
        if gid_128 in valid_gids:
            target_offset = 128
        else:
            for i in range(0, 200, 4):
                gid = struct.unpack_from('<H', raw, i)[0]
                if gid in valid_gids and i+220 < len(raw):
                    if struct.unpack_from('<H', raw, i+220)[0] in valid_gids:
                        target_offset = i
                        break
        # [Phase 2.2] æ€§èƒ½å¿«é€Ÿè·¯å¾„ï¼šæ ‡å‡†æ ¼å¼ (offset=36) æ— éœ€å¤„ç†
        if target_offset == 0 or target_offset == 36:
            return original_path
            
        if target_offset > 0:
            temp_dir = tempfile.gettempdir()
            temp_path = os.path.join(temp_dir, f"std_{os.path.basename(original_path)}")
            with open(original_path, 'rb') as src, open(temp_path, 'wb') as dst:
                dst.write(b'\x00' * 36) 
                src.seek(target_offset)
                shutil.copyfileobj(src, dst)
            return temp_path
        return original_path
    except Exception: return original_path

# ========================================================
# ğŸ§¹ [ç»ˆææ ¸å¿ƒ] V68.1 èåˆç‰ˆ (Strict ASCII + Semantic SNR)
# ========================================================
def is_semantic_garbage(text):
    """
    è¯­ä¹‰ä¿¡å™ªæ¯”æ£€æµ‹ (V67.7 æ ¸å¿ƒç®—æ³•)
    åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ç”±å¤§é‡çš„ ASCII ç¬¦å·ç»„æˆï¼ˆè§†è§‰ä¹±ç ï¼‰ã€‚
    ä¾‹å¦‚: "((m(*" æˆ– "\\;8\\;B" è™½ç„¶æ˜¯ ASCIIï¼Œä½†åº”è¢«å‰”é™¤ã€‚
    """
    if not text: return True
    
    # ç»Ÿè®¡å­—ç¬¦æ„æˆ
    alpha_num_count = 0  # å­—æ¯ã€æ•°å­—
    risky_sym_count = 0  # é£é™©ç¬¦å· (æ‹¬å·, æ–œæ , @, #, etc.)
    safe_sym_count = 0   # å®‰å…¨ç¬¦å· (ç©ºæ ¼, ., -, :)
    
    for char in text:
        if char.isalnum(): 
            alpha_num_count += 1
        elif char in " .-:/": 
            # è¿™äº›æ˜¯æ—¶é—´ã€æ•°å€¼ã€æ—¥æœŸä¸­å¸¸è§çš„å®‰å…¨ç¬¦å·ï¼Œä¸è®¡å…¥é£é™©
            safe_sym_count += 1
        else:
            # é£é™©ç¬¦å·ï¼š\ | ( ) [ ] { } < > ? ! @ # $ % ^ & * _ = + ; ' " ` ~
            risky_sym_count += 1
            
    total_len = len(text)
    
    # [é€»è¾‘ 1] æçŸ­å­—ç¬¦ä¸² (1-2å­—ç¬¦)
    # å¿…é¡»æ˜¯å­—æ¯æ•°å­—ï¼Œæˆ–è€…æ˜¯æ˜ç¡®çš„ç™½åå•å•å­—
    if total_len <= 2:
        # å¦‚æœåŒ…å«é£é™©ç¬¦å· (å¦‚ "m(" ) -> åˆ 
        if risky_sym_count > 0: return True
        
        # å•å­—æ¯/åŒå­—æ¯æ£€æŸ¥ (ç™½åå•æœºåˆ¶)
        # å…è®¸çº¯æ•°å­— (å¦‚ "1", "12")
        if text.isdigit(): return False
        
        # å…è®¸ç‰¹å®šå«ä¹‰çš„å­—æ¯ç»„åˆ (å¦‚ "A1", "V2")
        if text.isalnum() and any(c.isdigit() for c in text): return False
        
        # çº¯å­—æ¯æ£€æŸ¥ï¼šåªä¿ç•™å¸¸è§æ ‡è®°
        # V68.0 çš„ç™½åå•ï¼šA, V, P, R, T, S, M, I, W (æ³¢å½¢/å¯¼è”/äº‹ä»¶æ ‡è®°)
        if text.isalpha():
            if text.upper() not in ['A', 'V', 'P', 'R', 'T', 'S', 'M', 'I', 'W', 'L', 'B']:
                return True # "e", "q" ç­‰æ— æ„ä¹‰å­—æ¯è§†ä¸ºå™ªç‚¹
        
        return False

    # [é€»è¾‘ 2] ä¿¡å™ªæ¯”å¤±è¡¡ (ç¬¦å·æ¯”å­—å¤š)
    # ä¾‹å¦‚ "((m(*" -> Risky=4, Alpha=1 -> åˆ 
    # ä¾‹å¦‚ "\;8\;B" -> Risky=4, Alpha=3 -> åˆ 
    if risky_sym_count >= alpha_num_count and risky_sym_count > 1:
        return True
        
    # [é€»è¾‘ 3] ç¨€ç–å†…å®¹æ£€æµ‹
    # å¦‚æœæœ‰æ•ˆæ–‡å­—æå°‘ (<30%) ä¸”æ€»é•¿åº¦ > 4
    if total_len > 4:
        ratio = alpha_num_count / total_len
        if ratio < 0.3: return True
        
    return False

def clean_entries_content(entries):
    cleaned_list = []
    
    # ç³»ç»Ÿåº•å±‚æ•°æ®ç»„é»‘åå•
    GROUP_BLACKLIST = {
        'SYS', 'SYSTEM', 'DEBUG', 'DBG', 
        'UNK', 'UNKNOWN', 'IDK', '0',
        'ERROR', 'ERR', 'WARN', 
        'DATA', 'BLOB', 'ALARM'
    }
    
    for e in entries:
        raw_msg = str(e.message)
        raw_grp = str(e.group)

        # 1. [ç‰©ç†å±‚] Null æˆªæ–­ (æ¨¡æ‹Ÿ C å­—ç¬¦ä¸²)
        if '\x00' in raw_msg: raw_msg = raw_msg.split('\x00')[0]
        if '\x00' in raw_grp: raw_grp = raw_grp.split('\x00')[0]

        raw_msg = raw_msg.strip()
        raw_grp = raw_grp.strip()

        # 2. åŸºç¡€éç©ºæ ¡éªŒ
        if not raw_msg: continue
        if raw_grp.upper() in GROUP_BLACKLIST: continue

        # 3. [ç‰©ç†å±‚] Strict ASCII æ£€æµ‹ (V68.0 æ ¸å¿ƒ)
        # è‹±æ–‡è½¯ä»¶ä¸åº”åŒ…å«ä»»ä½• > 127 çš„å­—èŠ‚
        try:
            raw_msg.encode('ascii')
            raw_grp.encode('ascii')
        except UnicodeEncodeError:
            # åŒ…å«ä¹±ç å­—èŠ‚ -> ä¸¢å¼ƒ
            continue

        # 4. [ç‰©ç†å±‚] æ§åˆ¶ç¬¦æ£€æµ‹
        # è¿‡æ»¤ 0-31 çš„æ§åˆ¶ç¬¦ (ä¿ç•™ Tab, LF, CR)
        is_clean_ascii = True
        for char in raw_msg:
            code = ord(char)
            if code < 32 and code not in (9, 10, 13):
                is_clean_ascii = False
                break
        if not is_clean_ascii: continue

        # 5. [é€»è¾‘å±‚] è¯­ä¹‰ä¿¡å™ªæ¯”æ£€æµ‹ (V67.7 æ ¸å¿ƒå›å½’)
        # è¿‡æ»¤æ‰ "((m(*", "\;8\;B1", "#6#6" è¿™ç§ç”±åˆæ³• ASCII ç»„æˆçš„ä¹±ç 
        if is_semantic_garbage(raw_msg):
            continue

        # 6. ç»„è£…
        new_e = MutableEntry(
            timestamp=to_unix_seconds(e.timestamp),
            group=raw_grp,
            message=raw_msg,
            fid=getattr(e, 'fid', '0')
        )
        cleaned_list.append(new_e)

    cleaned_list.sort(key=lambda x: x.timestamp)
    return cleaned_list

# ========================================================
# ğŸ“ è¾…åŠ©å·¥å…·
# ========================================================
def get_raw_log_start_seconds(file_path):
    try:
        with open(file_path, 'rb') as f:
            raw = float(struct.unpack('<Q', f.read(8))[0])
            return to_unix_seconds(raw)
    except Exception: return 0.0

def get_safe_n_channels(header):
    try:
        if hasattr(header, 'n_channels'): return int(header.n_channels)
        if hasattr(header.amp, 'n_channels'): return int(header.amp.n_channels)
        if hasattr(header, 'channels'):
            if hasattr(header.channels, 'raw_mappings'): return len(header.channels.raw_mappings)
        return 0
    except Exception: return 0

def export_global_csv(entries, output_folder, study_id):
    try:
        filename = f"{study_id}_All_Entries_Normalized.csv"
        path = os.path.join(output_folder, study_id, filename)
        with open(path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['UnixSeconds', 'Group', 'Message'])
            for e in entries:
                writer.writerow([f"{e.timestamp:.3f}", e.group, e.message])
        return filename
    except Exception: return None

# --- æ ¸å¿ƒè½¬æ¢é€»è¾‘ ---
def execute_epycon_conversion(cfg):
    mem_handler = MemoryLogHandler()
    mem_handler.setFormatter(logging.Formatter('%(message)s')) # å†…å­˜æ—¥å¿—åªè®°å½•çº¯æ¶ˆæ¯
    
    # è·å–å…¨å±€å®šä¹‰çš„ logger
    conv_logger = logging.getLogger("epycon_web")
    conv_logger.setLevel(logging.DEBUG)  # ç¡®ä¿æ•è·æ‰€æœ‰çº§åˆ«
    conv_logger.propagate = False  # ä¸ä¼ æ’­åˆ°çˆ¶ loggerï¼Œåªç”¨æˆ‘ä»¬çš„å¤„ç†å™¨
    
    # [MODIFIED] ç°åœ¨æ¥å— task_id ä»¥æ›´æ–°çŠ¶æ€
    task_id = cfg.get("_task_id")
    
    def update_progress(p, log_msg=None):
        if task_id in TASKS:
            TASKS[task_id]['progress'] = p
            if log_msg:
                TASKS[task_id]['logs'].append(log_msg)

    # [FIX] å®šä¹‰ script_dir ä¾›é…ç½®åˆå§‹åŒ–ä½¿ç”¨
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ç¡®ä¿é…ç½®æ ¼å¼æ­£ç¡®
    cfg = _prepare_conversion_config(cfg, script_dir)
    input_folder = cfg["paths"]["input_folder"]
    output_folder = cfg["paths"]["output_folder"]
    
    conv_logger.info(f"ğŸ” æœ€ç»ˆè·¯å¾„: {input_folder}, exists={os.path.exists(input_folder)}")
    
    if not input_folder or not os.path.exists(input_folder):
        conv_logger.error(f"âŒ [v2024] è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
        update_progress(0, "è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        res_logs = mem_handler.logs
        conv_logger.removeHandler(mem_handler)
        return False, res_logs
    
    utf8_guard = UTF8EnforcedOpen()
    
    try:
        with utf8_guard:
            output_fmt = cfg["data"]["output_format"]
            # å…¼å®¹ "00000000" å’Œ "00000000.log" ä¸¤ç§æ ¼å¼
            valid_datalogs = set(
                f.rstrip(".log") if f.endswith(".log") else f
                for f in cfg["data"]["data_files"]
            )
            
            valid_studies = set(cfg["paths"].get("studies", []))
            study_list = _get_study_list(input_folder, valid_studies)
            
            if not study_list:
                conv_logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å­¦ä¹ æ–‡ä»¶ (study folders)ã€‚")
                update_progress(0, "æœªæ‰¾åˆ°ä»»ä½•å­¦ä¹ æ–‡ä»¶")
                res_logs = mem_handler.logs
                conv_logger.removeHandler(mem_handler)
                return False, res_logs
            
            if valid_studies:
                conv_logger.info(f"ğŸ“ å·²è¿‡æ»¤ studies: {len(study_list)} ä¸ªç¬¦åˆæ¡ä»¶")

            processed_count = 0
            
            # è·å–é…ç½®é€‰é¡¹
            merge_mode = cfg["data"].get("merge_logs", False)
            pseudonymize = cfg["global_settings"].get("pseudonymize", False)
            credentials = cfg["global_settings"].get("credentials", {})

            total_studies = len(study_list)
            for idx, study_path in enumerate(study_list):
                study_id = os.path.basename(study_path)
                # è®¡ç®—æ€»è¿›åº¦ç™¾åˆ†æ¯” (0-100)
                current_p = int((idx / total_studies) * 100)
                update_progress(current_p, f"æ­£åœ¨å¤„ç† study ({idx+1}/{total_studies}): {study_id}")
                
                logs_in_study = sorted(list(iglob(os.path.join(study_path, LOG_PATTERN))))
                if not logs_in_study: continue

                try: os.makedirs(os.path.join(output_folder, study_id), exist_ok=True)
                except Exception: pass
                
                # --- [Step 0] è¯»å– MASTER æ–‡ä»¶å¹¶å¤„ç†åŒ¿ååŒ– ---
                try:
                    master_info = _readmaster(os.path.join(study_path, MASTER_FILENAME))
                except (IOError, FileNotFoundError):
                    conv_logger.warning(f"âš ï¸ æœªæ‰¾åˆ° MASTER æ–‡ä»¶: {study_id}")
                    master_info = {"id": "", "name": ""}
                
                if pseudonymize:
                    tokenizer = Tokenize(8, {})
                    subject_id = tokenizer()
                    subject_name = ""
                    if master_info["id"]:
                        conv_logger.info(f"ğŸ”’ åŒ¿ååŒ–: {master_info['id']} -> {subject_id}")
                else:
                    subject_id = master_info["id"]
                    subject_name = master_info["name"]

                # --- [Step 1] è¯»å–å¹¶æ¸…æ´— Entries ---
                all_entries_norm = []
                epath = os.path.join(study_path, ENTRIES_FILENAME)
                need_entries = cfg["entries"]["convert"] or (cfg["data"]["output_format"] == "h5" and cfg["data"]["pin_entries"])
                conv_logger.info(f"ğŸ“‹ Entries é…ç½®: convert={cfg['entries']['convert']}, pin_entries={cfg['data']['pin_entries']}, need_entries={need_entries}")
                
                if need_entries:
                    if os.path.exists(epath):
                        try:
                            conv_logger.info(f"ğŸ” è¯»å–æ ‡æ³¨: {os.path.basename(epath)}")
                            clean_path = prepare_standard_entries_file(epath) 
                            native_entries = readentries(clean_path, version=cfg["global_settings"]["workmate_version"])
                            conv_logger.info(f"ğŸ“Š åŸå§‹æ ‡æ³¨æ¡æ•°: {len(native_entries)}")
                            
                            all_entries_norm = clean_entries_content(native_entries)
                            
                            if clean_path != epath and os.path.exists(clean_path):
                                try: os.remove(clean_path)
                                except Exception: pass
                                
                            conv_logger.info(f"âœ… å½’ä¸€åŒ–æ ‡æ³¨: {len(all_entries_norm)} æ¡ (ASCII+SNRåŒé‡å‡€åŒ–)")
                            export_global_csv(all_entries_norm, output_folder, study_id)
                        except Exception as e:
                            import traceback
                            conv_logger.warning(f"âš ï¸ è¯»å–å¤±è´¥: {e}\n{traceback.format_exc()}")
                    else:
                        conv_logger.info(f"â„¹ï¸ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {epath}")
                
                # --- [Step 1.5] å¯¼å‡ºæ±‡æ€» entries CSV (summary_csv) ---
                if cfg["entries"].get("summary_csv", False) and all_entries_norm:
                    try:
                        summary_path = os.path.join(output_folder, study_id, "entries_summary.csv")
                        entryplanter = EntryPlanter(all_entries_norm)
                        filter_groups = cfg["entries"].get("filter_annotation_type", [])
                        criteria = {
                            "fids": list(valid_datalogs) if valid_datalogs else [],
                            "groups": filter_groups if filter_groups else [],
                        }
                        entryplanter.savecsv(summary_path, criteria=criteria)
                        conv_logger.info(f"ğŸ“Š å¯¼å‡ºæ±‡æ€»æ ‡æ³¨: entries_summary.csv")
                    except Exception as e:
                        conv_logger.warning(f"âš ï¸ æ±‡æ€» CSV å¯¼å‡ºå¤±è´¥: {e}")

                # --- [Step 2] å¤„ç†æ•°æ®æ–‡ä»¶ ---
                # ç­›é€‰æœ‰æ•ˆçš„ datalog æ–‡ä»¶
                valid_logs = []
                for datalog_path in logs_in_study:
                    datalog_id = os.path.basename(datalog_path).replace(".log", "")
                    if valid_datalogs and datalog_id not in valid_datalogs: 
                        continue
                    valid_logs.append((datalog_path, datalog_id))
                
                if not valid_logs:
                    continue
                
                # ===================== åˆå¹¶æ¨¡å¼ =====================
                if merge_mode and output_fmt == "h5":
                    conv_logger.info(f"ğŸ“¦ åˆå¹¶æ¨¡å¼: å°† {len(valid_logs)} ä¸ªæ–‡ä»¶åˆå¹¶ä¸ºå•æ–‡ä»¶")
                    
                    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„æ—¶é—´æˆ³å’Œé€šé“ä¿¡æ¯
                    datalog_info = []
                    from epycon.core._dataclasses import Channels
                    from collections import defaultdict
                    
                    for datalog_path, datalog_id in valid_logs:
                        with LogParser(datalog_path, version=cfg["global_settings"]["workmate_version"], samplesize=1024) as p:
                            header = p.get_header()
                            if header is None:
                                conv_logger.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶å¤´: {datalog_id}.log")
                                continue
                            
                            # è·å–è¯¥æ–‡ä»¶çš„é€šé“æ˜ å°„
                            if cfg["data"]["leads"] == "computed":
                                if isinstance(header.channels, Channels):
                                    file_mappings = header.channels.computed_mappings
                                else:
                                    file_mappings = {f"ch{i}": [i] for i in range(header.num_channels)}
                            else:
                                if isinstance(header.channels, Channels):
                                    file_mappings = header.channels.raw_mappings
                                else:
                                    file_mappings = {f"ch{i}": [i] for i in range(header.num_channels)}
                            
                            if cfg["data"]["channels"]:
                                file_mappings = {k:v for k,v in file_mappings.items() if k in cfg["data"]["channels"]}
                            
                            datalog_info.append({
                                'path': datalog_path,
                                'id': datalog_id,
                                'timestamp': header.timestamp,
                                'header': header,
                                'mappings': file_mappings,
                                'num_output_channels': len(file_mappings),
                            })
                    
                    # æŒ‰æ—¶é—´æˆ³æ’åº
                    datalog_info.sort(key=lambda x: x['timestamp'])
                    
                    # æŒ‰é€šé“æ•°åˆ†ç»„
                    channel_groups = defaultdict(list)
                    for d in datalog_info:
                        channel_groups[d['num_output_channels']].append(d)
                    
                    conv_logger.info(f"âœ… é€šè¿‡éªŒè¯çš„æ–‡ä»¶æ•°: {len(datalog_info)}/{len(valid_logs)}")
                    
                    if len(channel_groups) > 1:
                        conv_logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸åŒé€šé“æ•°çš„æ–‡ä»¶ï¼Œå°†åˆ†ç»„å¤„ç†:")
                        for num_ch, files in channel_groups.items():
                            conv_logger.warning(f"   {num_ch} ä¸ªé€šé“: {len(files)} ä¸ªæ–‡ä»¶")
                    
                    # å¯¹æ¯ä¸ªé€šé“æ•°ç»„åˆ†åˆ«åˆå¹¶
                    for group_channel_count, group_files in channel_groups.items():
                        conv_logger.info(f"\nğŸ“¦ å¤„ç†é€šé“ç»„: {group_channel_count} ä¸ªé€šé“, {len(group_files)} ä¸ªæ–‡ä»¶")
                        
                        # è¯¥ç»„çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶å®šä¹‰åˆ—å
                        first_mappings = group_files[0]['mappings']
                        merged_column_names = list(first_mappings.keys())
                        first_timestamp = group_files[0]['timestamp']
                        
                        # æ„å»º HDF5 å…ƒæ•°æ®
                        hdf_attributes = {
                            "subject_id": subject_id,
                            "subject_name": subject_name,
                            "study_id": study_id,
                            "datalog_ids": ",".join([d['id'] for d in group_files]),
                            "timestamp": first_timestamp,
                            "datetime": datetime.fromtimestamp(first_timestamp).isoformat() if first_timestamp else "",
                            "merged": True,
                            "num_files": len(group_files),
                        }
                        if credentials:
                            hdf_attributes.update({
                                "author": credentials.get("author", ""),
                                "device": credentials.get("device", ""),
                                "owner": credentials.get("owner", ""),
                            })
                        
                        # åˆå¹¶è¾“å‡ºæ–‡ä»¶å
                        if len(channel_groups) > 1:
                            merged_output_path = os.path.join(output_folder, study_id, f"{study_id}_merged_{group_channel_count}ch.h5")
                        else:
                            merged_output_path = os.path.join(output_folder, study_id, f"{study_id}_merged.h5")
                        
                        is_first_file = True
                        total_samples = 0
                        
                        for idx, dlog_info in enumerate(group_files):
                            datalog_path = dlog_info['path']
                            datalog_id = dlog_info['id']
                            header = dlog_info['header']
                            fs = header.amp.sampling_freq
                            
                            processed_count += 1
                            conv_logger.info(f"   åˆå¹¶ {idx+1}/{len(group_files)}: {datalog_id}.log")
                            
                            # è®¡ç®—å½“å‰æ–‡ä»¶çš„æ—¶é—´èŒƒå›´
                            file_start_sec = float(header.timestamp)
                            n_channels = get_safe_n_channels(header)
                            file_size = os.path.getsize(datalog_path)
                            if n_channels > 0 and fs > 0:
                                n_samples = (file_size - 32) // (n_channels * 2)
                                file_duration_sec = n_samples / fs
                            else:
                                file_duration_sec = 0
                            file_end_sec = file_start_sec + file_duration_sec
                            
                            conv_logger.info(f"   â±ï¸ æ–‡ä»¶æ—¶é—´èŒƒå›´: {file_start_sec:.0f} - {file_end_sec:.2f} ({file_duration_sec:.3f}s)")
                            
                            # --- [æ ¸å¿ƒé€»è¾‘] ä¸¥æ ¼ FID åŒ¹é…æœºåˆ¶ ---
                            # ä»…é€‰æ‹© FID åŒ¹é…çš„æ–‡ä»¶ï¼Œç¡®ä¿æ ‡æ³¨å½’å± 100% å‡†ç¡®
                            file_entries = [e for e in all_entries_norm if str(e.fid) == str(datalog_id)]
                            conv_logger.info(f"   ğŸ“Š æ ‡æ³¨åŒ¹é…: {len(file_entries)} æ¡ (æŒ‰ FID ç­›é€‰)")
                            
                            with LogParser(
                                datalog_path, 
                                version=cfg["global_settings"]["workmate_version"], 
                                samplesize=cfg["global_settings"]["processing"]["chunk_size"]
                            ) as parser:
                                file_mappings = dlog_info['mappings']
                                
                                if is_first_file:
                                    hdf_attributes["sampling_freq"] = fs
                                    hdf_attributes["num_channels"] = len(merged_column_names)
                                
                                with HDFPlanter(
                                    merged_output_path,
                                    column_names=merged_column_names,
                                    sampling_freq=fs,
                                    factor=1000,
                                    units="mV",
                                    attributes=hdf_attributes if is_first_file else {},
                                    append=not is_first_file,
                                ) as planter:
                                    file_sample_count = 0
                                    for chunk in parser:
                                        chunk = mount_channels(chunk, file_mappings)
                                        planter.write(chunk)
                                        file_sample_count += chunk.shape[0]
                                        total_samples += chunk.shape[0]
                                    
                                    # ä¸ºè¿™ä¸ªæ–‡ä»¶åµŒå…¥å¯¹åº”çš„æ ‡æ³¨
                                    if cfg["data"]["pin_entries"] and file_entries:
                                        conv_logger.info(f"ğŸ“Œ æ–‡ä»¶ {datalog_id}: åµŒå…¥ {len(file_entries)} æ¡æ ‡æ³¨ (æ–‡ä»¶æ—¶é—´èŒƒå›´: {file_start_sec:.2f}-{file_end_sec:.2f})")
                                        
                                        global_base = total_samples - file_sample_count
                                        file_end_global = global_base + file_sample_count
                                        
                                        valid = []
                                        for e in file_entries:
                                            # è®¡ç®—ç›¸å¯¹äºå½“å‰ç»„èµ·å§‹æ—¶é—´çš„åç§» (ç§’)
                                            # æ³¨æ„ï¼šfirst_timestamp ä¸ºè¯¥ç»„ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„æ—¶é—´æˆ³åŸºå‡†
                                            offset_sec = e.timestamp - first_timestamp
                                            global_p = int(offset_sec * fs)
                                            
                                            # ä¸¥æ ¼æ ¡éªŒï¼šå¿…é¡»è½åœ¨å½“å‰æ–‡ä»¶çš„å…¨å±€é‡‡æ ·ç‚¹èŒƒå›´å†…
                                            if global_base <= global_p < file_end_global:
                                                valid.append((global_p, str(e.group), str(e.message)))
                                            else:
                                                conv_logger.warning(f"   âš ï¸ FID {datalog_id} åŒ¹é…ä½†æ—¶é—´æˆ³åç§» {offset_sec:.3f}s è½åœ¨æ–‡ä»¶èŒƒå›´ [{global_base/fs:.3f}, {file_end_global/fs:.3f}] ä¹‹å¤–")
                                        
                                        if valid:
                                            p, g, m = zip(*valid)
                                            planter.add_marks(list(p), list(g), list(m))
                                            conv_logger.info(f"   âœ… å·²å°† {len(valid)} æ¡æ ‡æ³¨ç²¾ç¡®åµŒå…¥")
                                        elif file_entries:
                                            conv_logger.warning(f"   âŒ {len(file_entries)} æ¡ FID åŒ¹é…çš„æ ‡æ³¨å‡å› æ—¶é—´èŒƒå›´ä¸ç¬¦è¢«å‰”é™¤ã€‚æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ï¼")
                            
                            is_first_file = False
                        
                        conv_logger.info(f"   âœ… åˆå¹¶å®Œæˆ: {merged_output_path} ({total_samples} samples)")
                
                else:
                    # ===================== å¸¸è§„æ¨¡å¼ (æ¯ä¸ªæ–‡ä»¶å•ç‹¬è¾“å‡º) =====================
                    for datalog_path, datalog_id in valid_logs:
                        processed_count += 1
                        conv_logger.info(f"å¤„ç†æ–‡ä»¶: {datalog_id}.log")
                        
                        try:
                            log_start_sec = get_raw_log_start_seconds(datalog_path)
                            
                            n_channels = 0
                            with LogParser(datalog_path, version=cfg["global_settings"]["workmate_version"], samplesize=1024) as p:
                                header = p.get_header()
                                if header is None:
                                    conv_logger.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶å¤´: {datalog_id}.log")
                                    continue
                                fs = header.amp.sampling_freq
                                n_channels = get_safe_n_channels(header)
                            
                            file_size = os.path.getsize(datalog_path)
                            duration_sec = 0.0
                            if n_channels > 0 and fs > 0:
                                n_samples = (file_size - 32) // (n_channels * 2)
                                duration_sec = n_samples / fs
                            
                            log_end_sec = log_start_sec + duration_sec
                            
                            # --- [æ ¸å¿ƒé€»è¾‘] ç‹¬ç«‹æ–‡ä»¶ä¸‹çš„ä¸¥æ ¼åŒ¹é… ---
                            target_entries_rel = [] 
                            for e in all_entries_norm:
                                if str(e.fid) == str(datalog_id):
                                    # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦è½åœ¨è¯¥æ–‡ä»¶çš„ç»å¯¹æ—¶é—´å†…
                                    if log_start_sec <= e.timestamp < log_end_sec:
                                        diff_seconds = e.timestamp - log_start_sec
                                        new_e = dataclasses.replace(e)
                                        new_e.timestamp = diff_seconds # è½¬æ¢ä¸ºç›¸å¯¹æ–‡ä»¶çš„åç§»ç§’æ•°
                                        target_entries_rel.append(new_e)
                                    else:
                                        conv_logger.warning(f"   âš ï¸ FID {datalog_id} åŒ¹é…ä½†æ ‡æ³¨æ—¶é—´æˆ³ {e.timestamp} æœªåœ¨æ­¤æ–‡ä»¶ç”Ÿå‘½å‘¨æœŸå†…")

                            # è½¬æ¢æ³¢å½¢
                            with LogParser(
                                datalog_path, 
                                version=cfg["global_settings"]["workmate_version"], 
                                samplesize=cfg["global_settings"]["processing"]["chunk_size"]
                            ) as parser:
                                # å¯¼å…¥ Channels ç±»ä»¥è¿›è¡Œç±»å‹æ£€æŸ¥
                                from epycon.core._dataclasses import Channels
                                
                                if cfg["data"]["leads"] == "computed":
                                    # header.channels ç°åœ¨æ˜¯ Channels å¯¹è±¡
                                    if isinstance(header.channels, Channels):
                                        mappings = header.channels.computed_mappings
                                    else:
                                        mappings = {f"ch{i}": [i] for i in range(header.num_channels)}
                                else:
                                    if isinstance(header.channels, Channels):
                                        mappings = header.channels.raw_mappings
                                    else:
                                        mappings = {f"ch{i}": [i] for i in range(header.num_channels)}
                                if cfg["data"]["channels"]:
                                    mappings = {k:v for k,v in mappings.items() if k in cfg["data"]["channels"]}
                                column_names = list(mappings.keys())
                                
                                entryplanter = EntryPlanter(target_entries_rel)
                                filter_groups = cfg["entries"]["filter_annotation_type"]
                                criteria = {"groups": filter_groups} if filter_groups else {}
                                
                                if file_fmt == "csv":
                                    entryplanter.savecsv(entry_out_path, criteria=criteria, ref_timestamp=0)
                                elif file_fmt == "sel":
                                    entryplanter.savesel(entry_out_path, 0, fs, list(mappings.keys()), criteria=criteria)
                                
                                conv_logger.info(f"   -> ğŸ“„ ç²¾ç¡®ç”Ÿæˆ: {datalog_id}.{file_fmt} ({len(target_entries_rel)}æ¡)")

                        except Exception as e:
                            conv_logger.error(f"âŒ æ–‡ä»¶ {datalog_id} è½¬æ¢å¤±è´¥: {str(e)}")
                            continue
                        
            update_progress(100, "âœ… è½¬æ¢åœ†æ»¡å®Œæˆ")
            conv_logger.info(f"âœ… å…¨éƒ¨å®Œæˆ! å…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
            res_logs = mem_handler.logs
            conv_logger.removeHandler(mem_handler)
            return True, res_logs
        
    except Exception as e:
        import traceback
        err = traceback.format_exc()
        conv_logger.error(f"âŒ ç³»ç»Ÿé”™è¯¯:\n{err}")
        res_logs = mem_handler.logs
        conv_logger.removeHandler(mem_handler)
        return False, res_logs

@app.route('/')
def home():
    """ è®¿é—®ä¸»é¡µä¸­å¿ƒ """
    html_path = resource_path('ui/index.html')
    if not os.path.exists(html_path):
        return f"UI é¦–é¡µç¼ºå¤±ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {html_path}", 404
        
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return render_template_string(content)
    except Exception as e:
        return f"æ— æ³•åŠ è½½é¦–é¡µ: {e}", 500

@app.route('/ui/<path:filename>')
def serve_ui(filename):
    """
    ç»Ÿä¸€å¤„ç† /ui/ è·¯å¾„ä¸‹çš„é™æ€èµ„äº§ã€‚
    åŒ…æ‹¬ HTMLï¼ˆè‡ªåŠ¨æ³¨å…¥å¯¼èˆªï¼‰ã€JSã€CSS å’Œå›¾åƒã€‚
    """
    import re
    from flask import make_response, send_from_directory
    
    ui_base = resource_path('ui')
    # å®‰å…¨æ€§æ£€æŸ¥ï¼šé¦–å…ˆæ¸…ç†æ–‡ä»¶åï¼Œé˜²æ­¢è·¯å¾„ç©¿è¶Š
    filename = secure_filename(filename)
    file_full_path = os.path.join(ui_base, filename)
    
    # è¿›ä¸€æ­¥ç¡®ä¿è·¯å¾„ä»åœ¨ ui_base ç›®å½•ä¸‹
    if not os.path.abspath(file_full_path).startswith(os.path.abspath(ui_base)):
        return "éæ³•çš„æ–‡ä»¶è¯·æ±‚", 403

    if not os.path.exists(file_full_path):
        return f"èµ„äº§æœªæ‰¾åˆ°: {filename}", 404
        
    # å¤„ç†é HTML é™æ€èµ„æº (tailwind.js, vue.js ç­‰)
    if not filename.lower().endswith('.html'):
        return send_from_directory(ui_base, filename)
        
    # å¤„ç†å­é¡µ HTML (è‡ªåŠ¨æ³¨å…¥è¿”å›ä¸»ä¸­å¿ƒçš„æŒ‰é’®)
    try:
        with open(file_full_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}", 500

    # ä»…å‘é index.html çš„ HTML æ–‡ä»¶æ³¨å…¥è¿”å›å¯¼èˆª
    if 'index.html' not in filename.lower():
        nav_injection = """
        <div id="epycon-home-nav" style="position:fixed; top:12px; right:12px; z-index:9999; opacity:0.9;">
            <a href="/" style="background:#0f172a; color:white; padding:8px 16px; border-radius:8px; text-decoration:none; font-size:13px; font-family:sans-serif; font-weight:500; box-shadow:0 4px 6px -1px rgba(0,0,0,0.1), 0 2px 4px -1px rgba(0,0,0,0.06); border:1px solid #334155;">
                â† è¿”å›æ•°æ®ä¸­å¿ƒ
            </a>
        </div>
        """
        # åœ¨ <body> æ ‡ç­¾åæ³¨å…¥
        body_match = re.search(r'<\s*body[^>]*>', content, re.IGNORECASE | re.DOTALL)
        if body_match:
            end_pos = body_match.end()
            content = content[:end_pos] + nav_injection + content[end_pos:]
        else:
            content = nav_injection + content
            
    response = make_response(content)
    response.headers['Content-Type'] = 'text/html; charset=utf-8'
    return response

@app.route('/vendor/<path:filename>')
def serve_vendor_compatibility(filename):
    """ 
    å…¼å®¹é€»è¾‘ï¼šå…è®¸æ ¹è·¯å¾„ä¸‹çš„ index.html é€šè¿‡ç›¸å¯¹è·¯å¾„ 'vendor/...' è®¿é—®èµ„æºã€‚
    è¿™ä½¿å¾— HTML åœ¨ç›´æ¥åŒå‡»æ‰“å¼€å’Œé€šè¿‡ Flask è®¿é—®æ—¶éƒ½èƒ½æ‰¾åˆ° CSS/JSã€‚
    """
    return send_from_directory(resource_path('ui/vendor'), filename)

@app.route('/<filename>.html')
def serve_html_compatibility(filename):
    """
    å…¼å®¹é€»è¾‘ï¼šå…è®¸æ ¹è·¯å¾„ä¸‹çš„è¯·æ±‚é‡å®šå‘åˆ° /ui/ è·¯å¾„ã€‚
    ä¾‹å¦‚è¯·æ±‚ /editor.html ä¼šæ˜ å°„åˆ° serve_ui('editor.html')
    """
    return serve_ui(f"{filename}.html")

@app.route('/run-direct', methods=['POST'])
def run_direct():
    # [Phase 2.1] JSON Schema éªŒè¯ - é˜²æ­¢æ— æ•ˆè¾“å…¥
    from jsonschema import validate, ValidationError
    
    CONFIG_API_SCHEMA = {
        "type": "object",
        "properties": {
            "paths": {
                "type": "object",
                "properties": {
                    "input_folder": {"type": "string"},
                    "output_folder": {"type": "string"},
                    "studies": {"type": "array", "items": {"type": "string"}}
                }
            },
            "data": {
                "type": "object",
                "properties": {
                    "output_format": {"type": "string", "enum": ["h5", "csv"]},
                    "merge_logs": {"type": "boolean"},
                    "pin_entries": {"type": "boolean"}
                }
            },
            "entries": {"type": "object"},
            "global_settings": {"type": "object"}
        }
    }
    
    try:
        config_data = request.json or {}
        
        # éªŒè¯è¾“å…¥ Schema
        try:
            validate(config_data, CONFIG_API_SCHEMA)
        except ValidationError as ve:
            return jsonify({
                "status": "error", 
                "message": f"é…ç½®æ ¼å¼é”™è¯¯: {ve.message}",
                "path": list(ve.path)
            }), 400
        
        task_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        TASKS[task_id] = {
            'status': 'running',
            'progress': 0,
            'logs': [],
            'result': None
        }
        
        config_data["_task_id"] = task_id # æ³¨å…¥ taskId
        
        # å¼‚æ­¥å¯åŠ¨è½¬æ¢
        def background_task():
            try:
                success, logs = execute_epycon_conversion(config_data)
                TASKS[task_id]['status'] = 'completed' if success else 'failed'
                TASKS[task_id]['result'] = {'success': success, 'logs': logs}
            except Exception as e:
                import traceback
                error_msg = f"Task backend error: {str(e)}\n{traceback.format_exc()}"
                TASKS[task_id]['status'] = 'failed'
                TASKS[task_id]['result'] = {'success': False, 'logs': [error_msg]}

        executor.submit(background_task)
        
        return jsonify({"status": "accepted", "task_id": task_id})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/api/task-status/<task_id>', methods=['GET'])
def get_task_status(task_id):
    task = TASKS.get(task_id)
    if not task:
        return jsonify({"status": "not_found"}), 404
    
    # æå–æ–°æ—¥å¿—å¹¶æ¸…ç©ºï¼ˆé˜²æ­¢é‡å¤ä¼ è¾“ï¼‰
    new_logs = task['logs']
    task['logs'] = []
    
    return jsonify({
        "status": task['status'],
        "progress": task['progress'],
        "logs": new_logs,
        "result": task['result']
    })

def _process_datalog_file(log_file, study_id, output_folder, cfg, conv_logger, planter_cls, valid_datalogs, all_entries_norm, leads):
    """
    å¤„ç†å•ä¸ª datalog æ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘
    """
    datalog_id = os.path.basename(log_file).replace(".log", "")
    merge_logs = cfg["data"].get("merge_logs", False)
    
    if not merge_logs and valid_datalogs and datalog_id not in valid_datalogs:
        return False

    conv_logger.info(f"ğŸ“„ Processing: {datalog_id} ...")
    parser = LogParser(log_file)
    
    # è½¬æ¢å¹¶å†™å…¥
    output_ext = ".h5" if planter_cls == HDFPlanter else ".csv"
    out_name = f"{datalog_id}{output_ext}"
    out_path = os.path.join(output_folder, study_id, out_name)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    
    # FID åŒ¹é…
    file_entries = [e for e in all_entries_norm if str(e.fid) == str(datalog_id)]
    
    with planter_cls(out_path, column_names=parser.headers) as planter:
        if isinstance(planter, HDFPlanter):
            planter.extra_attributes.update({"StudyID": study_id, "LogID": datalog_id, "Leads": leads})
            for chunk in parser.stream_data(chunk_size=cfg["global_settings"]["processing"]["chunk_size"]):
                planter.write(chunk)
            
            if cfg["data"]["pin_entries"] and file_entries:
                file_start_sec = parser.start_timestamp
                valid_marks = []
                for e in file_entries:
                    if file_start_sec <= e.timestamp:
                        offset_sec = e.timestamp - file_start_sec
                        sample_pos = int(offset_sec * parser.sampling_freq)
                        valid_marks.append((sample_pos, e.group, e.msg))
                
                if valid_marks:
                    p, g, m = zip(*valid_marks)
                    planter.add_marks(list(p), list(g), list(m))
        else:
            for chunk in parser.stream_data(chunk_size=cfg["global_settings"]["processing"]["chunk_size"]):
                planter.write(chunk)
    
    conv_logger.info(f"âœ¨ Finished: {out_name}")
    return True

# --- æ–°å¢çš„è¾…åŠ©åˆ†æ‹†å‡½æ•° ---
def _prepare_conversion_config(cfg, script_dir):
    if not isinstance(cfg, dict): cfg = {}
    if "paths" not in cfg or not isinstance(cfg["paths"], dict): cfg["paths"] = {}
    cfg["paths"].setdefault("input_folder", "examples/data")
    cfg["paths"].setdefault("output_folder", "examples/data/out")
    cfg["paths"].setdefault("studies", [])
    
    if "data" not in cfg or not isinstance(cfg["data"], dict): cfg["data"] = {}
    cfg["data"].setdefault("output_format", "h5")
    cfg["data"].setdefault("data_files", [])
    cfg["data"].setdefault("merge_logs", False)
    cfg["data"].setdefault("pin_entries", True)
    
    if "entries" not in cfg or not isinstance(cfg["entries"], dict): cfg["entries"] = {}
    cfg["entries"].setdefault("convert", False)
    cfg["entries"].setdefault("output_format", "csv")
    
    if "global_settings" not in cfg or not isinstance(cfg["global_settings"], dict): cfg["global_settings"] = {}
    cfg["global_settings"].setdefault("workmate_version", "4.3.2")
    cfg["global_settings"].setdefault("processing", {"chunk_size": 1024})

    # è·¯å¾„ç»å¯¹åŒ–
    if not os.path.isabs(cfg["paths"]["input_folder"]):
        cfg["paths"]["input_folder"] = os.path.normpath(os.path.join(script_dir, cfg["paths"]["input_folder"]))
    if not os.path.isabs(cfg["paths"]["output_folder"]):
        cfg["paths"]["output_folder"] = os.path.normpath(os.path.join(script_dir, cfg["paths"]["output_folder"]))
    return cfg

def _get_study_list(input_folder, valid_studies):
    direct_logs = list(iglob(os.path.join(input_folder, "*.log")))
    study_list = []
    if direct_logs:
        study_list.append(input_folder)
    else:
        for sub_path in iglob(os.path.join(input_folder, '**')):
            if os.path.isdir(sub_path):
                study_name = os.path.basename(sub_path)
                if valid_studies and study_name not in valid_studies:
                    continue
                # æ£€æŸ¥ç›®å½•å†…æ˜¯å¦å­˜åœ¨ log æ–‡ä»¶
                if any(iglob(os.path.join(sub_path, LOG_PATTERN))):
                    study_list.append(sub_path)
    return sorted(study_list)

@app.route('/api/select-folder', methods=['GET'])
def api_select_folder():
    """
    é€‰æ‹©æ–‡ä»¶å¤¹ï¼Œåœ¨ macOS ä¸Šä½¿ç”¨ AppleScript é¿å¼€çº¿ç¨‹å®‰å…¨é—®é¢˜ã€‚
    """
    try:
        path = ""
        if sys.platform == 'darwin':
            # macOS AppleScript é€»è¾‘
            cmd = ['osascript', '-e', 'tell application "System Events" to activate',
                   '-e', 'set theFolder to choose folder with prompt "è¯·é€‰æ‹©æ•°æ®è·¯å¾„:"',
                   '-e', 'POSIX path of theFolder']
            import subprocess
            res = subprocess.run(cmd, capture_output=True, text=True)
            if res.returncode == 0:
                path = res.stdout.strip()
        else:
            # Windows/Other Tkinter é€»è¾‘
            import tkinter as tk
            from tkinter import filedialog
            root = tk.Tk()
            root.withdraw()
            root.attributes('-topmost', True)
            path = filedialog.askdirectory()
            root.destroy()
            
        if path:
            path = os.path.normpath(path)
        return jsonify({"path": path})
    except Exception as e:
        return jsonify({"error": str(e), "path": ""})

@app.route('/api/shutdown', methods=['POST'])
def api_shutdown():
    """
    å…³é—­ Epycon GUI çš„ API ç«¯ç‚¹
    """
    try:
        response = jsonify({"status": "shutting_down", "message": "ç¨‹åºæ­£åœ¨å…³é—­..."})
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå…³é—­
        def shutdown_worker():
            time.sleep(0.5)  # ç­‰å¾… HTTP å“åº”å‘é€å®Œæ¯•
            cleanup_on_exit()
            # ä½¿ç”¨ os._exit(0) è€Œé sys.exit() æ˜¯å› ä¸ºï¼š
            # 1. æ­¤æ—¶åœ¨åå°çº¿ç¨‹ä¸­ï¼Œsys.exit() åªä¼šç»ˆæ­¢å½“å‰çº¿ç¨‹
            # 2. éœ€è¦å¼ºåˆ¶ç»ˆæ­¢æ•´ä¸ªè¿›ç¨‹ï¼ˆåŒ…æ‹¬ Flask ä¸»çº¿ç¨‹ï¼‰
            # 3. cleanup_on_exit() å·²åœ¨ä¸Šæ–¹æ‰‹åŠ¨è°ƒç”¨ï¼Œatexit å¤„ç†å™¨æ— éœ€å†æ‰§è¡Œ
            import os as os_module
            os_module._exit(0)
        
        shutdown_thread = threading.Thread(target=shutdown_worker, daemon=True)
        shutdown_thread.start()
        
        return response
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/api/restart', methods=['POST'])
def api_restart():
    """
    é‡å¯ Flask æœåŠ¡çš„ API ç«¯ç‚¹ã€‚
    è¿”å›æˆåŠŸåï¼Œå‰ç«¯ä¼šç­‰å¾…2ç§’å†åˆ·æ–°ã€‚
    """
    try:
        import subprocess
        import sys
        
        # ç«‹å³è¿”å›æˆåŠŸå“åº”
        response = jsonify({"status": "restarting", "message": "æœåŠ¡æ­£åœ¨é‡å¯ï¼Œè¯·ç¨å€™..."})
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œé‡å¯ï¼ˆä¸é˜»å¡å½“å‰è¯·æ±‚ï¼‰
        def restart_worker():
            import time
            time.sleep(1)  # ç­‰å¾… HTTP å“åº”å‘é€å®Œæ¯•
            
            # è·å–å½“å‰ç¯å¢ƒå˜é‡å¹¶æ¸…ç† Werkzeug/Reloader ç›¸å…³çš„å˜é‡ï¼Œé˜²æ­¢ Bad file descriptor é”™è¯¯
            new_env = os.environ.copy()
            for key in ['WERKZEUG_RUN_MAIN', 'WERKZEUG_SERVER_FD']:
                new_env.pop(key, None)
            
            # ä¿ç•™ EPYCON_ACTUAL_PORT è®©æ–°è¿›ç¨‹å°è¯•å›æ”¶æ—§ç«¯å£
            # åœ¨åå°å¯åŠ¨æ–°çš„ app_gui.py è¿›ç¨‹
            subprocess.Popen([sys.executable, 'app_gui.py'], cwd=os.getcwd(), env=new_env)
            
            # ç­‰å¾…æ–°è¿›ç¨‹å¯åŠ¨åï¼Œå…³é—­å½“å‰è¿›ç¨‹
            time.sleep(2)
            import os as os_module
            os_module._exit(0)
        
        restart_thread = threading.Thread(target=restart_worker, daemon=True)
        restart_thread.start()
        
        return response
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def open_browser(port=5000):
    try:
        url = f"http://127.0.0.1:{port}/"
        logging.getLogger(__name__).info(f"Opening browser to {url}")
        if os.environ.get('NO_BROWSER') != '1':
            webbrowser.open(url)
        else:
            print(f"è·³è¿‡æ‰“å¼€æµè§ˆå™¨ (NO_BROWSER=1)ï¼Œè¯·æ‰‹åŠ¨è®¿é—®: {url}")
    except Exception as e:
        logging.getLogger(__name__).error(f"Failed to open browser: {e}")
        print(f"è¯·æ‰‹åŠ¨æ‰“å¼€æµè§ˆå™¨è®¿é—®: {url}")

if __name__ == '__main__':
    try:
        # ç¡®ä¿å·¥ä½œç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        os.chdir(script_dir)
        
        # è¯†åˆ«æ˜¯å¦ä¸ºæ‰“åŒ…åçš„ EXE
        is_frozen = getattr(sys, 'frozen', False)
        
        for stream in (sys.stdout, sys.stderr):
            # Use a concrete type check so static analyzers (Pylance) know this
            # object supports `reconfigure`. `io.TextIOWrapper` exposes
            # reconfigure() on Python 3.7+.
            if isinstance(stream, io.TextIOWrapper):
                try:
                    stream.reconfigure(encoding="utf-8", errors="replace")
                except Exception:
                    pass
        
        # 1. ç«¯å£ç®¡ç†
        # è¯†åˆ«æ˜¯å¦æ˜¯ Flask Reloader çš„å­å·¥ä½œè¿›ç¨‹
        is_worker = os.environ.get('WERKZEUG_RUN_MAIN') == 'true'
        
        # å°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ç«¯å£ï¼ˆé€šå¸¸æ˜¯é‡å¯æ—¶ä¼ é€’ï¼‰ï¼Œå¦åˆ™é»˜è®¤ 5000
        env_port = os.environ.get('EPYCON_ACTUAL_PORT')
        preferred_port = int(env_port) if env_port else 5000
        port = preferred_port
        
        # ä»…åœ¨é Worker è¿›ç¨‹ä¸­è¿›è¡Œç«¯å£æ¢æµ‹å’Œå†²çªæ¸…ç†
        # Worker è¿›ç¨‹åº”å½“ç›´æ¥ä¿¡ä»»å¹¶ä½¿ç”¨çˆ¶è¿›ç¨‹åˆ†é…çš„ç«¯å£
        if not is_worker:
            if not check_port_available(port):
                # å°è¯•æ¸…ç†å ç”¨è€…ï¼ˆä¾‹å¦‚é‡å¯æ—¶çš„æ—§å®ä¾‹ï¼‰
                success, occupier = kill_port_occupier(port)
                
                # å¦‚æœæ˜¯ç³»ç»ŸæœåŠ¡ï¼Œæˆ–è€…æ¸…ç†å¤±è´¥ï¼Œåˆ™æœç´¢æ–°ç«¯å£
                if not success or not check_port_available(port):
                    msg = f"âš ï¸  ç«¯å£ {port} è¢«å ç”¨"
                    if occupier != "None": msg += f" ({occupier})"
                    print(f"{msg}ï¼Œæ­£åœ¨æœç´¢å¯ç”¨ç«¯å£...")
                    
                    found = False
                    for p in range(5000, 5051): # ä» 5000 å¼€å§‹é‡æ–°æœç´¢
                        if check_port_available(p):
                            port = p
                            found = True
                            print(f"âœ… å·²é€‰æ‹©å¯ç”¨ç«¯å£: {port}")
                            break
                    if not found:
                        print("âŒ æœªæ‰¾åˆ° 5000-5050 èŒƒå›´å†…çš„å¯ç”¨ç«¯å£ã€‚")
                        input("\næŒ‰å›è½¦é”®é€€å‡º...")
                        sys.exit(1)
        
        # å­˜å…¥ç¯å¢ƒå˜é‡ï¼Œä¾›å­è¿›ç¨‹å’Œé‡å¯åçš„å®ä¾‹ä½¿ç”¨
        os.environ['EPYCON_ACTUAL_PORT'] = str(port)

        # æ£€æŸ¥æ˜¯å¦æ˜¯ Flask Reloader çš„çˆ¶è¿›ç¨‹
        is_reloader_parent = (not is_frozen and not is_worker)
        
        # å•å®ä¾‹æ£€æŸ¥å¿…é¡»åœ¨ Reloader çˆ¶è¿›ç¨‹ä¸­æ‰§è¡Œï¼ˆé˜²æ­¢å¤šä¸ªå®ä¾‹å¯åŠ¨ï¼‰
        print("ğŸ” æ­£åœ¨è¿›è¡Œå¯åŠ¨å‰æ£€æŸ¥...")
        if not check_single_instance():
            print("\nâŒ ç¨‹åºå·²åœ¨è¿è¡Œï¼Œæ— æ³•å¯åŠ¨æ–°å®ä¾‹ã€‚")
            print("æç¤ºï¼šå¦‚æœç¡®è®¤æ²¡æœ‰å…¶ä»–å®ä¾‹ï¼Œè¯·åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š")
            print(f"      {os.path.join(tempfile.gettempdir(), 'epycon_gui.lock')}")
            input("\næŒ‰å›è½¦é”®é€€å‡º...")
            sys.exit(1)
        
        print("âœ… å¯åŠ¨æ£€æŸ¥é€šè¿‡")

        # å¦‚æœä»¥ PyInstaller æ‰“åŒ…ä¸º EXE å¹¶åœ¨ Windows ä¸Šè¿è¡Œï¼Œæœ€å°åŒ–æ§åˆ¶å°çª—å£
        try:
            if is_frozen and os.name == 'nt':
                import ctypes
                SW_MINIMIZE = 6
                hWnd = ctypes.windll.kernel32.GetConsoleWindow()
                if hWnd:
                    ctypes.windll.user32.ShowWindow(hWnd, SW_MINIMIZE)
        except Exception:
            pass

        print("\nğŸš€ Epycon GUI (V68.3) å¯åŠ¨ä¸­...")
        print("ğŸ“Œ PID:", os.getpid())
        print(f"ğŸŒ è®¿é—®åœ°å€: http://127.0.0.1:{port}/")
        print("ğŸ’¡ æç¤º: å¯åœ¨é¡µé¢ä¸­ç‚¹å‡»'é€€å‡ºç¨‹åº'æŒ‰é’®å…³é—­ï¼Œæˆ–æŒ‰ Ctrl+C é€€å‡º\n")
        
        # æ³¨å†Œä¿¡å·å¤„ç†ï¼ˆä¼˜é›…é€€å‡ºï¼‰
        def signal_handler(sig, frame):
            print("\n\nğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
            cleanup_on_exit()
            print("âœ… æ¸…ç†å®Œæˆï¼Œç¨‹åºå·²é€€å‡º")
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        if hasattr(signal, 'SIGTERM'):
            signal.signal(signal.SIGTERM, signal_handler)
        
        # å¯¹äº EXE ç‰ˆæœ¬ï¼Œç¦ç”¨ reloaderï¼ˆé¿å…è¿›ç¨‹ç®¡ç†é—®é¢˜ï¼‰
        use_reloader = not is_frozen
        
        # ä»…åœ¨å·¥ä½œè¿›ç¨‹ä¸­æ‰“å¼€æµè§ˆå™¨ï¼Œé¿å… reloader å¯¼è‡´æ‰“å¼€ä¸¤æ¬¡
        # WERKZEUG_RUN_MAIN='true' è¡¨ç¤ºè¿™æ˜¯ Flask çš„å®é™…å·¥ä½œè¿›ç¨‹
        # å¯åŠ¨æµè§ˆå™¨é€»è¾‘
        # å½“ reloader ç¦ç”¨æ—¶ï¼Œç›´æ¥å¯åŠ¨ï¼›å½“ reloader å¯ç”¨æ—¶ï¼Œä»…åœ¨å·¥ä½œè¿›ç¨‹ä¸­å¯åŠ¨
        should_open = is_frozen or os.environ.get('WERKZEUG_RUN_MAIN') == 'true' or not use_reloader
        
        if should_open:
            threading.Thread(
                target=lambda: (time.sleep(2), open_browser(port)),
                daemon=True
            ).start()
            
        # å¯åŠ¨æœåŠ¡å™¨
        # ä¿®æ­£ï¼šé»˜è®¤ç»‘å®šåˆ° 127.0.0.1 ä»¥é˜²æ­¢å±€åŸŸç½‘å¤–éƒ¨è®¿é—®
        # å¦‚æœç¡®å®éœ€è¦è¿œç¨‹è®¿é—®ï¼Œè¯·é€šè¿‡ç¯å¢ƒå˜é‡é…ç½® EPYCON_HOST=0.0.0.0
        host_ip = os.environ.get('EPYCON_HOST', '127.0.0.1')
        app.run(
            host=host_ip,
            port=port,
            debug=not is_frozen, 
            use_reloader=use_reloader,
            threaded=True
        )
    except Exception as e:
        print(f"å¯åŠ¨é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()