import os
import sys
import threading
import time
import webbrowser
import logging
import io
import tkinter as tk
from tkinter import filedialog, messagebox
from flask import Flask, request, jsonify, send_file, send_from_directory, make_response, render_template_string
from flask_cors import CORS
from glob import iglob
import dataclasses
import struct
import csv
import tempfile
import shutil
from datetime import datetime, timezone
import socket
import atexit
import signal

# ========================================================
# ğŸ›¡ï¸ è¿è¡Œæ—¶ç¯å¢ƒ
# ========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
candidates = [current_dir, os.path.join(current_dir, "epycon")]
for path in candidates:
    if os.path.exists(path) and path not in sys.path:
        sys.path.insert(0, path)

def resource_path(relative_path):
    """ è·å–èµ„æºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå…¼å®¹å¼€å‘ç¯å¢ƒä¸ PyInstaller æ‰“åŒ…ç¯å¢ƒ """
    try:
        base_path = sys._MEIPASS  # type: ignore
    except Exception:
        # æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½• current_dirï¼Œè€Œä¸æ˜¯è¿è¡Œæ—¶çš„ CWD
        base_path = current_dir
    return os.path.join(base_path, relative_path)

# ========================================================
# ğŸ”’ å•å®ä¾‹æ£€æŸ¥å’Œç«¯å£ç®¡ç†
# ========================================================
LOCK_FILE = None

def check_single_instance():
    """æ£€æŸ¥æ˜¯å¦å·²æœ‰å®ä¾‹åœ¨è¿è¡Œ"""
    global LOCK_FILE
    lock_path = os.path.join(tempfile.gettempdir(), 'epycon_gui.lock')
    current_pid = os.getpid()
    is_subprocess = os.environ.get('WERKZEUG_RUN_MAIN') == 'true'
    
    try:
        # å°è¯•åˆ›å»ºé”æ–‡ä»¶
        if os.path.exists(lock_path):
            # æ£€æŸ¥é”æ–‡ä»¶ä¸­çš„ PID æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            try:
                with open(lock_path, 'r') as f:
                    lock_data = f.read().strip().split(',')
                    old_pid = int(lock_data[0])
                    lock_parent_pid = int(lock_data[1]) if len(lock_data) > 1 else None
                
                # å¦‚æœå½“å‰è¿›ç¨‹æ˜¯ Reloader çš„å­è¿›ç¨‹ï¼Œä¸”çˆ¶è¿›ç¨‹ PID ç›¸åŒï¼Œåˆ™å…è®¸
                if is_subprocess and lock_parent_pid is not None:
                    parent_pid = os.getppid() if hasattr(os, 'getppid') else None
                    if parent_pid == lock_parent_pid:
                        # è¿™æ˜¯åŒä¸€ä¸ª Reloader å¯åŠ¨çš„å­è¿›ç¨‹ï¼Œå…è®¸ç»§ç»­
                        return True
                
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
                if os.name == 'nt':
                    try:
                        import psutil
                        # æ£€æŸ¥ old_pid å’Œé”æ–‡ä»¶ä¸­çš„çˆ¶è¿›ç¨‹æ˜¯å¦éƒ½è¿˜æ´»ç€
                        if psutil.pid_exists(old_pid):
                            print(f"âš ï¸ æ£€æµ‹åˆ°å¦ä¸€ä¸ªå®ä¾‹æ­£åœ¨è¿è¡Œ (PID: {old_pid})")
                            print("è¯·å…ˆå…³é—­å…¶ä»–å®ä¾‹ï¼Œæˆ–ç­‰å¾…å‡ ç§’åé‡è¯•ã€‚")
                            return False
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰ psutilï¼Œä½¿ç”¨ç®€å•çš„æ—¶é—´æ£€æŸ¥
                        file_age = time.time() - os.path.getmtime(lock_path)
                        if file_age < 60:  # å¦‚æœé”æ–‡ä»¶åœ¨ 1 åˆ†é’Ÿå†…åˆ›å»ºï¼Œè®¤ä¸ºè¿˜åœ¨ä½¿ç”¨
                            # ä½†å¦‚æœæˆ‘ä»¬æ˜¯å­è¿›ç¨‹ä¸”çˆ¶ PID ç›¸åŒï¼Œåˆ™å…è®¸
                            if not (is_subprocess and lock_parent_pid is not None):
                                print(f"âš ï¸ æ£€æµ‹åˆ°é”æ–‡ä»¶ (åˆ›å»ºäº {int(file_age)} ç§’å‰)")
                                print("å¦‚æœç¡®è®¤æ²¡æœ‰å…¶ä»–å®ä¾‹è¿è¡Œï¼Œè¯·æ‰‹åŠ¨åˆ é™¤é”æ–‡ä»¶ï¼š")
                                print(f"   {lock_path}")
                                return False
            except (ValueError, IOError):
                pass
            
            # å¦‚æœè¿›ç¨‹ä¸å­˜åœ¨ï¼Œåˆ é™¤æ—§é”æ–‡ä»¶
            try:
                os.remove(lock_path)
            except:
                pass
        
        # å¦‚æœè¿™æ˜¯ Reloader çš„å­è¿›ç¨‹ï¼Œä¸è¦é‡æ–°åˆ›å»ºé”æ–‡ä»¶
        if is_subprocess:
            return True
        
        # åˆ›å»ºæ–°é”æ–‡ä»¶ï¼ˆè®°å½•çˆ¶è¿›ç¨‹ PID ç”¨äº Reloader è¯†åˆ«ï¼‰
        parent_pid = os.getpid()  # åœ¨ä¸»è¿›ç¨‹ä¸­ï¼Œè‡ªå·±å°±æ˜¯"çˆ¶"
        LOCK_FILE = open(lock_path, 'w')
        LOCK_FILE.write(f"{parent_pid},{parent_pid}")  # æ ¼å¼: current_pid, parent_pid
        LOCK_FILE.flush()
        
        # Windows ä¸Šå°è¯•åŠ é”
        if os.name == 'nt':
            import msvcrt
            try:
                msvcrt.locking(LOCK_FILE.fileno(), msvcrt.LK_NBLCK, 1)
            except:
                pass
        
        return True
    except Exception as e:
        print(f"å•å®ä¾‹æ£€æŸ¥å¤±è´¥: {e}")
        return True  # å‡ºé”™æ—¶å…è®¸ç»§ç»­è¿è¡Œ

def check_port_available(port=5000):
    """æ£€æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        sock.bind(('127.0.0.1', port))
        sock.close()
        return True
    except OSError:
        return False

def kill_port_occupier(port=5000):
    """å°è¯•ç»ˆæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹"""
    if os.name != 'nt':
        return False
    
    try:
        import subprocess
        # æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
        result = subprocess.run(
            ['netstat', '-ano'],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        for line in result.stdout.split('\n'):
            if f':{port}' in line and 'LISTENING' in line:
                parts = line.split()
                if parts:
                    pid = parts[-1]
                    try:
                        pid = int(pid)
                        print(f"ğŸ”ª æ­£åœ¨ç»ˆæ­¢å ç”¨ç«¯å£ {port} çš„è¿›ç¨‹ (PID: {pid})...")
                        subprocess.run(['taskkill', '/F', '/PID', str(pid)], timeout=5)
                        time.sleep(2)
                        return True
                    except:
                        pass
    except Exception as e:
        print(f"ç»ˆæ­¢ç«¯å£å ç”¨è¿›ç¨‹å¤±è´¥: {e}")
    return False

def cleanup_on_exit():
    """ç¨‹åºé€€å‡ºæ—¶çš„æ¸…ç†å·¥ä½œ"""
    global LOCK_FILE
    if LOCK_FILE:
        try:
            LOCK_FILE.close()
            lock_path = os.path.join(tempfile.gettempdir(), 'epycon_gui.lock')
            if os.path.exists(lock_path):
                os.remove(lock_path)
        except:
            pass

# æ³¨å†Œé€€å‡ºæ¸…ç†
atexit.register(cleanup_on_exit)

# ========================================================
# ğŸ”§ å¼ºåˆ¶ UTF-8 å†™å…¥
# ========================================================
try:
    import builtins
    _real_open = builtins.open
    class UTF8EnforcedOpen:
        def __enter__(self):
            self.original_open = builtins.open
            def new_open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):
                if 'w' in mode or 'a' in mode:
                    encoding = 'utf-8'
                    errors = 'replace' 
                return self.original_open(file, mode, buffering, encoding, errors, newline, closefd, opener)
            builtins.open = new_open
        def __exit__(self, exc_type, exc_val, exc_tb):
            builtins.open = self.original_open
except ImportError: pass

# ========================================================
# ğŸ“¦ å¯¼å…¥ Epycon
# ========================================================
try:
    from epycon.config.byteschema import ENTRIES_FILENAME, LOG_PATTERN, MASTER_FILENAME
    from epycon.iou import LogParser, EntryPlanter, CSVPlanter, HDFPlanter, readentries, mount_channels
    from epycon.iou.parsers import _readmaster
    from epycon.utils.person import Tokenize
except ImportError as e:
    print(f"æ— æ³•åŠ è½½ Epyconã€‚\n{e}")
    sys.exit(1)

app = Flask(__name__, static_folder='.', static_url_path='')
CORS(app)

# ========================================================
# ğŸ“ [æ ¸å¿ƒ] å…¨å±€æ—¥å¿—é…ç½® (åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°)
# ========================================================
LOG_PATH = os.path.join(tempfile.gettempdir(), "epycon_gui.log")
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_PATH, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("epycon_web")

class MemoryLogHandler(logging.Handler):
    def __init__(self):
        super().__init__()
        self.logs = []
        self.setLevel(logging.DEBUG)  # æ•è·æ‰€æœ‰çº§åˆ«çš„æ—¥å¿—
    def emit(self, record):
        self.logs.append(self.format(record))

# ========================================================
# ğŸ—ï¸ [æ ¸å¿ƒ] è‡ªå®šä¹‰å¯å˜ Entry å¯¹è±¡
# ========================================================
@dataclasses.dataclass
class MutableEntry:
    timestamp: float
    group: str
    message: str
    fid: str = '0'
    duration: float = 0
    color: int = 0

# ========================================================
# âš–ï¸ [æ ¸å¿ƒ] å…¨è‡ªåŠ¨æ—¶é—´å½’ä¸€åŒ– (Unix Seconds)
# ========================================================
def to_unix_seconds(val):
    try:
        if isinstance(val, datetime):
            return val.replace(tzinfo=timezone.utc).timestamp()
        num = float(val)
        if num > 100_000_000_000_000_000: # FILETIME
            return (num - 116444736000000000) / 10_000_000.0
        if num > 100_000_000_000: # Milliseconds
            return num / 1000.0
        return num
    except:
        return 0.0

# ========================================================
# ğŸ› ï¸ [æ ¸å¿ƒ] entries.log å»å£³
# ========================================================
def prepare_standard_entries_file(original_path):
    try:
        with open(original_path, 'rb') as f:
            raw = f.read(256)
        valid_gids = [1, 2, 3, 4, 5, 6, 17]
        target_offset = 0
        gid_128 = struct.unpack_from('<H', raw, 128)[0]
        if gid_128 in valid_gids:
            target_offset = 128
        else:
            for i in range(0, 200, 4):
                gid = struct.unpack_from('<H', raw, i)[0]
                if gid in valid_gids and i+220 < len(raw):
                    if struct.unpack_from('<H', raw, i+220)[0] in valid_gids:
                        target_offset = i
                        break
        if target_offset > 0 and target_offset != 36:
            temp_dir = tempfile.gettempdir()
            temp_path = os.path.join(temp_dir, f"std_{os.path.basename(original_path)}")
            with open(original_path, 'rb') as src, open(temp_path, 'wb') as dst:
                dst.write(b'\x00' * 36) 
                src.seek(target_offset)
                shutil.copyfileobj(src, dst)
            return temp_path
        return original_path
    except: return original_path

# ========================================================
# ğŸ§¹ [ç»ˆææ ¸å¿ƒ] V68.1 èåˆç‰ˆ (Strict ASCII + Semantic SNR)
# ========================================================
def is_semantic_garbage(text):
    """
    è¯­ä¹‰ä¿¡å™ªæ¯”æ£€æµ‹ (V67.7 æ ¸å¿ƒç®—æ³•)
    åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ç”±å¤§é‡çš„ ASCII ç¬¦å·ç»„æˆï¼ˆè§†è§‰ä¹±ç ï¼‰ã€‚
    ä¾‹å¦‚: "((m(*" æˆ– "\\;8\\;B" è™½ç„¶æ˜¯ ASCIIï¼Œä½†åº”è¢«å‰”é™¤ã€‚
    """
    if not text: return True
    
    # ç»Ÿè®¡å­—ç¬¦æ„æˆ
    alpha_num_count = 0  # å­—æ¯ã€æ•°å­—
    risky_sym_count = 0  # é£é™©ç¬¦å· (æ‹¬å·, æ–œæ , @, #, etc.)
    safe_sym_count = 0   # å®‰å…¨ç¬¦å· (ç©ºæ ¼, ., -, :)
    
    for char in text:
        if char.isalnum(): 
            alpha_num_count += 1
        elif char in " .-:/": 
            # è¿™äº›æ˜¯æ—¶é—´ã€æ•°å€¼ã€æ—¥æœŸä¸­å¸¸è§çš„å®‰å…¨ç¬¦å·ï¼Œä¸è®¡å…¥é£é™©
            safe_sym_count += 1
        else:
            # é£é™©ç¬¦å·ï¼š\ | ( ) [ ] { } < > ? ! @ # $ % ^ & * _ = + ; ' " ` ~
            risky_sym_count += 1
            
    total_len = len(text)
    
    # [é€»è¾‘ 1] æçŸ­å­—ç¬¦ä¸² (1-2å­—ç¬¦)
    # å¿…é¡»æ˜¯å­—æ¯æ•°å­—ï¼Œæˆ–è€…æ˜¯æ˜ç¡®çš„ç™½åå•å•å­—
    if total_len <= 2:
        # å¦‚æœåŒ…å«é£é™©ç¬¦å· (å¦‚ "m(" ) -> åˆ 
        if risky_sym_count > 0: return True
        
        # å•å­—æ¯/åŒå­—æ¯æ£€æŸ¥ (ç™½åå•æœºåˆ¶)
        # å…è®¸çº¯æ•°å­— (å¦‚ "1", "12")
        if text.isdigit(): return False
        
        # å…è®¸ç‰¹å®šå«ä¹‰çš„å­—æ¯ç»„åˆ (å¦‚ "A1", "V2")
        if text.isalnum() and any(c.isdigit() for c in text): return False
        
        # çº¯å­—æ¯æ£€æŸ¥ï¼šåªä¿ç•™å¸¸è§æ ‡è®°
        # V68.0 çš„ç™½åå•ï¼šA, V, P, R, T, S, M, I, W (æ³¢å½¢/å¯¼è”/äº‹ä»¶æ ‡è®°)
        if text.isalpha():
            if text.upper() not in ['A', 'V', 'P', 'R', 'T', 'S', 'M', 'I', 'W', 'L', 'B']:
                return True # "e", "q" ç­‰æ— æ„ä¹‰å­—æ¯è§†ä¸ºå™ªç‚¹
        
        return False

    # [é€»è¾‘ 2] ä¿¡å™ªæ¯”å¤±è¡¡ (ç¬¦å·æ¯”å­—å¤š)
    # ä¾‹å¦‚ "((m(*" -> Risky=4, Alpha=1 -> åˆ 
    # ä¾‹å¦‚ "\;8\;B" -> Risky=4, Alpha=3 -> åˆ 
    if risky_sym_count >= alpha_num_count and risky_sym_count > 1:
        return True
        
    # [é€»è¾‘ 3] ç¨€ç–å†…å®¹æ£€æµ‹
    # å¦‚æœæœ‰æ•ˆæ–‡å­—æå°‘ (<30%) ä¸”æ€»é•¿åº¦ > 4
    if total_len > 4:
        ratio = alpha_num_count / total_len
        if ratio < 0.3: return True
        
    return False

def clean_entries_content(entries):
    cleaned_list = []
    
    # ç³»ç»Ÿåº•å±‚æ•°æ®ç»„é»‘åå•
    GROUP_BLACKLIST = {
        'SYS', 'SYSTEM', 'DEBUG', 'DBG', 
        'UNK', 'UNKNOWN', 'IDK', 
        'ERROR', 'ERR', 'WARN', 
        'DATA', 'BLOB', 'ALARM'
    }
    
    for e in entries:
        raw_msg = str(e.message)
        raw_grp = str(e.group)

        # 1. [ç‰©ç†å±‚] Null æˆªæ–­ (æ¨¡æ‹Ÿ C å­—ç¬¦ä¸²)
        if '\x00' in raw_msg: raw_msg = raw_msg.split('\x00')[0]
        if '\x00' in raw_grp: raw_grp = raw_grp.split('\x00')[0]

        raw_msg = raw_msg.strip()
        raw_grp = raw_grp.strip()

        # 2. åŸºç¡€éç©ºæ ¡éªŒ
        if not raw_msg: continue
        if raw_grp.upper() in GROUP_BLACKLIST: continue

        # 3. [ç‰©ç†å±‚] Strict ASCII æ£€æµ‹ (V68.0 æ ¸å¿ƒ)
        # è‹±æ–‡è½¯ä»¶ä¸åº”åŒ…å«ä»»ä½• > 127 çš„å­—èŠ‚
        try:
            raw_msg.encode('ascii')
            raw_grp.encode('ascii')
        except UnicodeEncodeError:
            # åŒ…å«ä¹±ç å­—èŠ‚ -> ä¸¢å¼ƒ
            continue

        # 4. [ç‰©ç†å±‚] æ§åˆ¶ç¬¦æ£€æµ‹
        # è¿‡æ»¤ 0-31 çš„æ§åˆ¶ç¬¦ (ä¿ç•™ Tab, LF, CR)
        is_clean_ascii = True
        for char in raw_msg:
            code = ord(char)
            if code < 32 and code not in (9, 10, 13):
                is_clean_ascii = False
                break
        if not is_clean_ascii: continue

        # 5. [é€»è¾‘å±‚] è¯­ä¹‰ä¿¡å™ªæ¯”æ£€æµ‹ (V67.7 æ ¸å¿ƒå›å½’)
        # è¿‡æ»¤æ‰ "((m(*", "\;8\;B1", "#6#6" è¿™ç§ç”±åˆæ³• ASCII ç»„æˆçš„ä¹±ç 
        if is_semantic_garbage(raw_msg):
            continue

        # 6. ç»„è£…
        new_e = MutableEntry(
            timestamp=to_unix_seconds(e.timestamp),
            group=raw_grp,
            message=raw_msg
        )
        cleaned_list.append(new_e)

    cleaned_list.sort(key=lambda x: x.timestamp)
    return cleaned_list

# ========================================================
# ğŸ“ è¾…åŠ©å·¥å…·
# ========================================================
def get_raw_log_start_seconds(file_path):
    try:
        with open(file_path, 'rb') as f:
            raw = float(struct.unpack('<Q', f.read(8))[0])
            return to_unix_seconds(raw)
    except: return 0.0

def get_safe_n_channels(header):
    try:
        if hasattr(header, 'n_channels'): return int(header.n_channels)
        if hasattr(header.amp, 'n_channels'): return int(header.amp.n_channels)
        if hasattr(header, 'channels'):
            if hasattr(header.channels, 'raw_mappings'): return len(header.channels.raw_mappings)
        return 0
    except: return 0

def export_global_csv(entries, output_folder, study_id):
    try:
        filename = f"{study_id}_All_Entries_Normalized.csv"
        path = os.path.join(output_folder, study_id, filename)
        with open(path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['UnixSeconds', 'Group', 'Message'])
            for e in entries:
                writer.writerow([f"{e.timestamp:.3f}", e.group, e.message])
        return filename
    except: return None

# --- æ ¸å¿ƒè½¬æ¢é€»è¾‘ ---
def execute_epycon_conversion(cfg):
    mem_handler = MemoryLogHandler()
    mem_handler.setFormatter(logging.Formatter('%(message)s')) # å†…å­˜æ—¥å¿—åªè®°å½•çº¯æ¶ˆæ¯
    
    # è·å–å…¨å±€å®šä¹‰çš„ logger
    conv_logger = logging.getLogger("epycon_web")
    conv_logger.setLevel(logging.DEBUG)  # ç¡®ä¿æ•è·æ‰€æœ‰çº§åˆ«
    conv_logger.propagate = False  # ä¸ä¼ æ’­åˆ°çˆ¶ loggerï¼Œåªç”¨æˆ‘ä»¬çš„å¤„ç†å™¨
    
    # ä¸´æ—¶æ·»åŠ å†…å­˜å¤„ç†å™¨ï¼Œä»»åŠ¡ç»“æŸåç§»é™¤
    conv_logger.addHandler(mem_handler)
    
    # ç¡®ä¿å·¥ä½œç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•ï¼ˆåœ¨ Flask ç¯å¢ƒä¸­å¯èƒ½ä¼šå˜åŒ–ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)
    
    # ç¡®ä¿æ‰€æœ‰é…ç½®é”®éƒ½å­˜åœ¨ï¼ˆå…³é”®ï¼ï¼‰
    if not isinstance(cfg, dict):
        cfg = {}
    
    if "paths" not in cfg or not isinstance(cfg["paths"], dict):
        cfg["paths"] = {}
    if "input_folder" not in cfg["paths"]:
        cfg["paths"]["input_folder"] = "examples/data"
    if "output_folder" not in cfg["paths"]:
        cfg["paths"]["output_folder"] = "examples/data/out"
    cfg["paths"].setdefault("studies", [])
    
    if "data" not in cfg or not isinstance(cfg["data"], dict):
        cfg["data"] = {}
    cfg["data"].setdefault("output_format", "h5")
    cfg["data"].setdefault("data_files", [])
    cfg["data"].setdefault("channels", [])
    cfg["data"].setdefault("custom_channels", {})
    cfg["data"].setdefault("leads", "original")
    cfg["data"].setdefault("merge_logs", False)
    cfg["data"].setdefault("pin_entries", True)  # é»˜è®¤å¯ç”¨ï¼šåµŒå…¥æ ‡æ³¨åˆ° H5 æ–‡ä»¶
    
    if "entries" not in cfg or not isinstance(cfg["entries"], dict):
        cfg["entries"] = {}
    cfg["entries"].setdefault("filter_annotation_type", [])
    cfg["entries"].setdefault("convert", False)
    cfg["entries"].setdefault("output_format", "csv")
    cfg["entries"].setdefault("summary_csv", False)
    
    if "global_settings" not in cfg or not isinstance(cfg["global_settings"], dict):
        cfg["global_settings"] = {}
    cfg["global_settings"].setdefault("credentials", {})
    cfg["global_settings"].setdefault("workmate_version", "4.3.2")
    cfg["global_settings"].setdefault("pseudonymize", False)
    cfg["global_settings"].setdefault("processing", {})
    if "processing" not in cfg["global_settings"] or not isinstance(cfg["global_settings"]["processing"], dict):
        cfg["global_settings"]["processing"] = {}
    cfg["global_settings"]["processing"].setdefault("chunk_size", 1024)
    
    # è½¬æ¢ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
    input_folder_raw = cfg["paths"]["input_folder"]
    output_folder_raw = cfg["paths"]["output_folder"]
    
    conv_logger.info(f"ğŸ” è·¯å¾„è½¬æ¢å‰: input={input_folder_raw}, is_abs={os.path.isabs(input_folder_raw)}")
    
    if not os.path.isabs(input_folder_raw):
        cfg["paths"]["input_folder"] = os.path.join(script_dir, input_folder_raw)
        conv_logger.info(f"âœ… è·¯å¾„å·²è½¬æ¢: {input_folder_raw} -> {cfg['paths']['input_folder']}")
    if not os.path.isabs(output_folder_raw):
        cfg["paths"]["output_folder"] = os.path.join(script_dir, output_folder_raw)
    
    # ç°åœ¨éªŒè¯è½¬æ¢åçš„è·¯å¾„
    input_folder = cfg["paths"]["input_folder"]
    output_folder = cfg["paths"]["output_folder"]
    
    conv_logger.info(f"ğŸ” æœ€ç»ˆè·¯å¾„: {input_folder}, exists={os.path.exists(input_folder)}")
    
    if not input_folder or not os.path.exists(input_folder):
        conv_logger.error(f"âŒ [v2024] è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
        res_logs = mem_handler.logs
        conv_logger.removeHandler(mem_handler)
        return False, res_logs
    
    utf8_guard = UTF8EnforcedOpen()
    
    try:
        with utf8_guard:
            output_fmt = cfg["data"]["output_format"]
            # å…¼å®¹ "00000000" å’Œ "00000000.log" ä¸¤ç§æ ¼å¼
            valid_datalogs = set(
                f.rstrip(".log") if f.endswith(".log") else f
                for f in cfg["data"]["data_files"]
            )
            
            # è·å– studies è¿‡æ»¤åˆ—è¡¨
            valid_studies = set(cfg["paths"].get("studies", []))
            
            direct_logs = list(iglob(os.path.join(input_folder, "*.log")))
            study_list = []
            if direct_logs:
                study_list.append(input_folder)
            else:
                for sub_path in iglob(os.path.join(input_folder, '**')):
                    if os.path.isdir(sub_path):
                        # åº”ç”¨ studies è¿‡æ»¤
                        study_name = os.path.basename(sub_path)
                        if valid_studies and study_name not in valid_studies:
                            continue
                        study_list.append(sub_path)
            
            if not study_list:
                conv_logger.warning("âš ï¸ æœªæ‰¾åˆ° log æ–‡ä»¶ã€‚")
                res_logs = mem_handler.logs
                conv_logger.removeHandler(mem_handler)
                return False, res_logs
            
            if valid_studies:
                conv_logger.info(f"ğŸ“ å·²è¿‡æ»¤ studies: {len(study_list)} ä¸ªç¬¦åˆæ¡ä»¶")

            processed_count = 0
            
            # è·å–é…ç½®é€‰é¡¹
            merge_mode = cfg["data"].get("merge_logs", False)
            pseudonymize = cfg["global_settings"].get("pseudonymize", False)
            credentials = cfg["global_settings"].get("credentials", {})

            for study_path in study_list:
                study_id = os.path.basename(study_path)
                logs_in_study = sorted(list(iglob(os.path.join(study_path, LOG_PATTERN))))
                if not logs_in_study: continue

                try: os.makedirs(os.path.join(output_folder, study_id), exist_ok=True)
                except: pass
                
                # --- [Step 0] è¯»å– MASTER æ–‡ä»¶å¹¶å¤„ç†åŒ¿ååŒ– ---
                try:
                    master_info = _readmaster(os.path.join(study_path, MASTER_FILENAME))
                except (IOError, FileNotFoundError):
                    conv_logger.warning(f"âš ï¸ æœªæ‰¾åˆ° MASTER æ–‡ä»¶: {study_id}")
                    master_info = {"id": "", "name": ""}
                
                if pseudonymize:
                    tokenizer = Tokenize(8, {})
                    subject_id = tokenizer()
                    subject_name = ""
                    if master_info["id"]:
                        conv_logger.info(f"ğŸ”’ åŒ¿ååŒ–: {master_info['id']} -> {subject_id}")
                else:
                    subject_id = master_info["id"]
                    subject_name = master_info["name"]

                # --- [Step 1] è¯»å–å¹¶æ¸…æ´— Entries ---
                all_entries_norm = []
                epath = os.path.join(study_path, ENTRIES_FILENAME)
                need_entries = cfg["entries"]["convert"] or (cfg["data"]["output_format"] == "h5" and cfg["data"]["pin_entries"])
                conv_logger.info(f"ğŸ“‹ Entries é…ç½®: convert={cfg['entries']['convert']}, pin_entries={cfg['data']['pin_entries']}, need_entries={need_entries}")
                
                if need_entries:
                    if os.path.exists(epath):
                        try:
                            conv_logger.info(f"ğŸ” è¯»å–æ ‡æ³¨: {os.path.basename(epath)}")
                            clean_path = prepare_standard_entries_file(epath) 
                            native_entries = readentries(clean_path, version=cfg["global_settings"]["workmate_version"])
                            conv_logger.info(f"ğŸ“Š åŸå§‹æ ‡æ³¨æ¡æ•°: {len(native_entries)}")
                            
                            all_entries_norm = clean_entries_content(native_entries)
                            
                            if clean_path != epath and os.path.exists(clean_path):
                                try: os.remove(clean_path)
                                except: pass
                                
                            conv_logger.info(f"âœ… å½’ä¸€åŒ–æ ‡æ³¨: {len(all_entries_norm)} æ¡ (ASCII+SNRåŒé‡å‡€åŒ–)")
                            export_global_csv(all_entries_norm, output_folder, study_id)
                        except Exception as e:
                            import traceback
                            conv_logger.warning(f"âš ï¸ è¯»å–å¤±è´¥: {e}\n{traceback.format_exc()}")
                    else:
                        conv_logger.info(f"â„¹ï¸ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {epath}")
                
                # --- [Step 1.5] å¯¼å‡ºæ±‡æ€» entries CSV (summary_csv) ---
                if cfg["entries"].get("summary_csv", False) and all_entries_norm:
                    try:
                        summary_path = os.path.join(output_folder, study_id, "entries_summary.csv")
                        entryplanter = EntryPlanter(all_entries_norm)
                        filter_groups = cfg["entries"].get("filter_annotation_type", [])
                        criteria = {
                            "fids": list(valid_datalogs) if valid_datalogs else [],
                            "groups": filter_groups if filter_groups else [],
                        }
                        entryplanter.savecsv(summary_path, criteria=criteria)
                        conv_logger.info(f"ğŸ“Š å¯¼å‡ºæ±‡æ€»æ ‡æ³¨: entries_summary.csv")
                    except Exception as e:
                        conv_logger.warning(f"âš ï¸ æ±‡æ€» CSV å¯¼å‡ºå¤±è´¥: {e}")

                # --- [Step 2] å¤„ç†æ•°æ®æ–‡ä»¶ ---
                # ç­›é€‰æœ‰æ•ˆçš„ datalog æ–‡ä»¶
                valid_logs = []
                for datalog_path in logs_in_study:
                    datalog_id = os.path.basename(datalog_path).replace(".log", "")
                    if valid_datalogs and datalog_id not in valid_datalogs: 
                        continue
                    valid_logs.append((datalog_path, datalog_id))
                
                if not valid_logs:
                    continue
                
                # ===================== åˆå¹¶æ¨¡å¼ =====================
                if merge_mode and output_fmt == "h5":
                    conv_logger.info(f"ğŸ“¦ åˆå¹¶æ¨¡å¼: å°† {len(valid_logs)} ä¸ªæ–‡ä»¶åˆå¹¶ä¸ºå•æ–‡ä»¶")
                    
                    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„æ—¶é—´æˆ³å’Œé€šé“ä¿¡æ¯
                    datalog_info = []
                    from epycon.core._dataclasses import Channels
                    from collections import defaultdict
                    
                    for datalog_path, datalog_id in valid_logs:
                        with LogParser(datalog_path, version=cfg["global_settings"]["workmate_version"], samplesize=1024) as p:
                            header = p.get_header()
                            if header is None:
                                conv_logger.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶å¤´: {datalog_id}.log")
                                continue
                            
                            # è·å–è¯¥æ–‡ä»¶çš„é€šé“æ˜ å°„
                            if cfg["data"]["leads"] == "computed":
                                if isinstance(header.channels, Channels):
                                    file_mappings = header.channels.computed_mappings
                                else:
                                    file_mappings = {f"ch{i}": [i] for i in range(header.num_channels)}
                            else:
                                if isinstance(header.channels, Channels):
                                    file_mappings = header.channels.raw_mappings
                                else:
                                    file_mappings = {f"ch{i}": [i] for i in range(header.num_channels)}
                            
                            if cfg["data"]["channels"]:
                                file_mappings = {k:v for k,v in file_mappings.items() if k in cfg["data"]["channels"]}
                            
                            datalog_info.append({
                                'path': datalog_path,
                                'id': datalog_id,
                                'timestamp': header.timestamp,
                                'header': header,
                                'mappings': file_mappings,
                                'num_output_channels': len(file_mappings),
                            })
                    
                    # æŒ‰æ—¶é—´æˆ³æ’åº
                    datalog_info.sort(key=lambda x: x['timestamp'])
                    
                    # æŒ‰é€šé“æ•°åˆ†ç»„
                    channel_groups = defaultdict(list)
                    for d in datalog_info:
                        channel_groups[d['num_output_channels']].append(d)
                    
                    conv_logger.info(f"âœ… é€šè¿‡éªŒè¯çš„æ–‡ä»¶æ•°: {len(datalog_info)}/{len(valid_logs)}")
                    
                    if len(channel_groups) > 1:
                        conv_logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸åŒé€šé“æ•°çš„æ–‡ä»¶ï¼Œå°†åˆ†ç»„å¤„ç†:")
                        for num_ch, files in channel_groups.items():
                            conv_logger.warning(f"   {num_ch} ä¸ªé€šé“: {len(files)} ä¸ªæ–‡ä»¶")
                    
                    # å¯¹æ¯ä¸ªé€šé“æ•°ç»„åˆ†åˆ«åˆå¹¶
                    for group_channel_count, group_files in channel_groups.items():
                        conv_logger.info(f"\nğŸ“¦ å¤„ç†é€šé“ç»„: {group_channel_count} ä¸ªé€šé“, {len(group_files)} ä¸ªæ–‡ä»¶")
                        
                        # è¯¥ç»„çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶å®šä¹‰åˆ—å
                        first_mappings = group_files[0]['mappings']
                        merged_column_names = list(first_mappings.keys())
                        first_timestamp = group_files[0]['timestamp']
                        
                        # æ„å»º HDF5 å…ƒæ•°æ®
                        hdf_attributes = {
                            "subject_id": subject_id,
                            "subject_name": subject_name,
                            "study_id": study_id,
                            "datalog_ids": ",".join([d['id'] for d in group_files]),
                            "timestamp": first_timestamp,
                            "datetime": datetime.fromtimestamp(first_timestamp).isoformat() if first_timestamp else "",
                            "merged": True,
                            "num_files": len(group_files),
                        }
                        if credentials:
                            hdf_attributes.update({
                                "author": credentials.get("author", ""),
                                "device": credentials.get("device", ""),
                                "owner": credentials.get("owner", ""),
                            })
                        
                        # åˆå¹¶è¾“å‡ºæ–‡ä»¶å
                        if len(channel_groups) > 1:
                            merged_output_path = os.path.join(output_folder, study_id, f"{study_id}_merged_{group_channel_count}ch.h5")
                        else:
                            merged_output_path = os.path.join(output_folder, study_id, f"{study_id}_merged.h5")
                        
                        is_first_file = True
                        total_samples = 0
                        
                        for idx, dlog_info in enumerate(group_files):
                            datalog_path = dlog_info['path']
                            datalog_id = dlog_info['id']
                            header = dlog_info['header']
                            fs = header.amp.sampling_freq
                            
                            processed_count += 1
                            conv_logger.info(f"   åˆå¹¶ {idx+1}/{len(group_files)}: {datalog_id}.log")
                            
                            # è®¡ç®—å½“å‰æ–‡ä»¶çš„æ—¶é—´èŒƒå›´
                            file_start_sec = float(header.timestamp)
                            n_channels = get_safe_n_channels(header)
                            file_size = os.path.getsize(datalog_path)
                            if n_channels > 0 and fs > 0:
                                n_samples = (file_size - 32) // (n_channels * 2)
                                file_duration_sec = n_samples / fs
                            else:
                                file_duration_sec = 0
                            file_end_sec = file_start_sec + file_duration_sec
                            
                            conv_logger.info(f"   â±ï¸ æ–‡ä»¶æ—¶é—´èŒƒå›´: {file_start_sec:.0f} - {file_end_sec:.2f} ({file_duration_sec:.3f}s)")
                            
                            # ç­›é€‰è¿™ä¸ªæ–‡ä»¶å¯¹åº”çš„æ ‡æ³¨
                            is_last_file = (idx == len(group_files) - 1)
                            if is_last_file:
                                file_entries = [e for e in all_entries_norm if file_start_sec <= e.timestamp <= file_end_sec]
                            else:
                                file_entries = [e for e in all_entries_norm if file_start_sec <= e.timestamp < file_end_sec]
                            conv_logger.info(f"   ğŸ“Š æ ‡æ³¨åŒ¹é…: {len(file_entries)}/{len(all_entries_norm)} ç¬¦åˆæ—¶é—´èŒƒå›´ (æœ€åä¸€ä¸ªæ–‡ä»¶: {is_last_file})")
                            
                            with LogParser(
                                datalog_path, 
                                version=cfg["global_settings"]["workmate_version"], 
                                samplesize=cfg["global_settings"]["processing"]["chunk_size"]
                            ) as parser:
                                file_mappings = dlog_info['mappings']
                                
                                if is_first_file:
                                    hdf_attributes["sampling_freq"] = fs
                                    hdf_attributes["num_channels"] = len(merged_column_names)
                                
                                with HDFPlanter(
                                    merged_output_path,
                                    column_names=merged_column_names,
                                    sampling_freq=fs,
                                    factor=1000,
                                    units="mV",
                                    attributes=hdf_attributes if is_first_file else {},
                                    append=not is_first_file,
                                ) as planter:
                                    file_sample_count = 0
                                    for chunk in parser:
                                        chunk = mount_channels(chunk, file_mappings)
                                        planter.write(chunk)
                                        file_sample_count += chunk.shape[0]
                                        total_samples += chunk.shape[0]
                                    
                                    # ä¸ºè¿™ä¸ªæ–‡ä»¶åµŒå…¥å¯¹åº”çš„æ ‡æ³¨
                                    if cfg["data"]["pin_entries"] and file_entries:
                                        conv_logger.info(f"ğŸ“Œ æ–‡ä»¶ {datalog_id}: åµŒå…¥ {len(file_entries)} æ¡æ ‡æ³¨ (æ–‡ä»¶æ—¶é—´èŒƒå›´: {file_start_sec:.2f}-{file_end_sec:.2f})")
                                        
                                        global_base = total_samples - file_sample_count
                                        file_end_global = global_base + file_sample_count
                                        
                                        valid = []
                                        for e in file_entries:
                                            relative_pos = round((e.timestamp - file_start_sec) * fs)
                                            global_p = global_base + relative_pos
                                            
                                            if global_base <= global_p < file_end_global:
                                                valid.append((global_p, str(e.group), str(e.message)))
                                            else:
                                                conv_logger.debug(f"   âš ï¸ æ ‡æ³¨è¶…å‡ºèŒƒå›´: ts={e.timestamp}, rel_pos={relative_pos}, global_pos={global_p}, valid_range=[{global_base}, {file_end_global})")
                                        
                                        conv_logger.info(f"   æ ‡æ³¨éªŒè¯: {len(file_entries)} â†’ {len(valid)} æœ‰æ•ˆ (base={global_base}, samples={file_sample_count}, end={file_end_global})")
                                        if valid:
                                            p, g, m = zip(*valid)
                                            planter.add_marks(list(p), list(g), list(m))
                                            conv_logger.info(f"   âœ… å·²å°† {len(valid)} æ¡æ ‡æ³¨åµŒå…¥")
                                        elif len(file_entries) > 0:
                                            conv_logger.warning(f"   âš ï¸ {len(file_entries)} æ¡æ ‡æ³¨éƒ½è¶…å‡ºæœ‰æ•ˆèŒƒå›´ï¼")
                            
                            is_first_file = False
                        
                        conv_logger.info(f"   âœ… åˆå¹¶å®Œæˆ: {merged_output_path} ({total_samples} samples)")
                
                else:
                    # ===================== å¸¸è§„æ¨¡å¼ (æ¯ä¸ªæ–‡ä»¶å•ç‹¬è¾“å‡º) =====================
                    for datalog_path, datalog_id in valid_logs:
                        processed_count += 1
                        conv_logger.info(f"å¤„ç†æ–‡ä»¶: {datalog_id}.log")
                        
                        try:
                            log_start_sec = get_raw_log_start_seconds(datalog_path)
                            
                            n_channels = 0
                            with LogParser(datalog_path, version=cfg["global_settings"]["workmate_version"], samplesize=1024) as p:
                                header = p.get_header()
                                if header is None:
                                    conv_logger.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶å¤´: {datalog_id}.log")
                                    continue
                                fs = header.amp.sampling_freq
                                n_channels = get_safe_n_channels(header)
                            
                            file_size = os.path.getsize(datalog_path)
                            duration_sec = 0.0
                            if n_channels > 0 and fs > 0:
                                n_samples = (file_size - 32) // (n_channels * 2)
                                duration_sec = n_samples / fs
                            
                            log_end_sec = log_start_sec + duration_sec
                            
                            # é€‰æ‹©è¯¥æ–‡ä»¶å¯¹åº”çš„æ ‡æ³¨ï¼ˆä½¿ç”¨é—­åŒºé—´åŒ…æ‹¬è¾¹ç•Œï¼‰
                            # åœ¨å¸¸è§„æ¨¡å¼ä¸­ï¼Œæ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹å¤„ç†ï¼Œæ‰€ä»¥ä½¿ç”¨é—­åŒºé—´æ˜¯å®‰å…¨çš„
                            target_entries_rel = [] 
                            for e in all_entries_norm:
                                if log_start_sec <= e.timestamp <= log_end_sec:
                                    diff_seconds = e.timestamp - log_start_sec
                                    new_e = dataclasses.replace(e)
                                    new_e.timestamp = diff_seconds
                                    target_entries_rel.append(new_e)

                            # è½¬æ¢æ³¢å½¢
                            with LogParser(
                                datalog_path, 
                                version=cfg["global_settings"]["workmate_version"], 
                                samplesize=cfg["global_settings"]["processing"]["chunk_size"]
                            ) as parser:
                                # å¯¼å…¥ Channels ç±»ä»¥è¿›è¡Œç±»å‹æ£€æŸ¥
                                from epycon.core._dataclasses import Channels
                                
                                if cfg["data"]["leads"] == "computed":
                                    # header.channels ç°åœ¨æ˜¯ Channels å¯¹è±¡
                                    if isinstance(header.channels, Channels):
                                        mappings = header.channels.computed_mappings
                                    else:
                                        mappings = {f"ch{i}": [i] for i in range(header.num_channels)}
                                else:
                                    if isinstance(header.channels, Channels):
                                        mappings = header.channels.raw_mappings
                                    else:
                                        mappings = {f"ch{i}": [i] for i in range(header.num_channels)}
                                if cfg["data"]["channels"]:
                                    mappings = {k:v for k,v in mappings.items() if k in cfg["data"]["channels"]}
                                column_names = list(mappings.keys())
                                
                                out_path = os.path.join(output_folder, study_id, f"{datalog_id}.{output_fmt}")
                                
                                # æ„å»º HDF5 å…ƒæ•°æ®ï¼ˆéåˆå¹¶æ¨¡å¼ï¼‰
                                hdf_attributes = {
                                    "subject_id": subject_id,
                                    "subject_name": subject_name,
                                    "study_id": study_id,
                                    "datalog_id": datalog_id,
                                    "timestamp": header.timestamp,
                                    "datetime": datetime.fromtimestamp(header.timestamp).isoformat() if header.timestamp else "",
                                }
                                if credentials:
                                    hdf_attributes.update({
                                        "author": credentials.get("author", ""),
                                        "device": credentials.get("device", ""),
                                        "owner": credentials.get("owner", ""),
                                    })
                                
                                if output_fmt == "csv":
                                    PlanterClass = CSVPlanter
                                    planter_kwargs = {"column_names": column_names, "sampling_freq": fs}
                                else:
                                    PlanterClass = HDFPlanter
                                    planter_kwargs = {
                                        "column_names": column_names, 
                                        "sampling_freq": fs,
                                        "factor": 1000,
                                        "units": "mV",
                                        "attributes": hdf_attributes,
                                    }
                                
                                with PlanterClass(out_path, **planter_kwargs) as planter:
                                    for chunk in parser:
                                        chunk = mount_channels(chunk, mappings)
                                        planter.write(chunk)
                                        
                                    if output_fmt == "h5" and cfg["data"]["pin_entries"] and target_entries_rel:
                                        if isinstance(planter, HDFPlanter):
                                            conv_logger.info(f"ğŸ“Œ å¼€å§‹åµŒå…¥æ ‡æ³¨: å…± {len(target_entries_rel)} æ¡ (å¸¸è§„æ¨¡å¼)")
                                            conv_logger.info(f"   é‡‡æ ·ç‡: {fs} Hz")
                                            
                                            # target_entries_rel å·²ç»æ˜¯ç›¸å¯¹æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè½¬æ¢ä¸ºæ ·æœ¬ä½ç½®
                                            valid = []
                                            for e in target_entries_rel:
                                                # ä½¿ç”¨ round è€Œä¸æ˜¯ intï¼Œæ›´ç²¾ç¡®
                                                sample_pos = round(e.timestamp * fs)
                                                # éªŒè¯ä½ç½®æœ‰æ•ˆæ€§ï¼šå¿…é¡»æ˜¯éè´Ÿæ•°
                                                if sample_pos >= 0:
                                                    valid.append((sample_pos, str(e.group), str(e.message)))
                                                else:
                                                    conv_logger.debug(f"   âš ï¸ æ ‡æ³¨ä½ç½®æ— æ•ˆ: ts={e.timestamp}s, pos={sample_pos}")
                                            
                                            conv_logger.info(f"âœ… æ ‡æ³¨ä½ç½®è®¡ç®—å®Œæˆ: æœ‰æ•ˆæ¡æ•° {len(valid)}/{len(target_entries_rel)}")
                                            if valid:
                                                pos_range = [v[0] for v in valid]
                                                conv_logger.info(f"   è®¡ç®—å¾—åˆ°çš„ä½ç½®èŒƒå›´: {min(pos_range)} - {max(pos_range)} é‡‡æ ·ç‚¹")
                                                p, g, m = zip(*valid)
                                                planter.add_marks(list(p), list(g), list(m))
                                                conv_logger.info(f"ğŸ“ å·²å°† {len(valid)} æ¡æ ‡æ³¨åµŒå…¥ H5 æ–‡ä»¶")
                                            elif len(target_entries_rel) > 0:
                                                conv_logger.warning(f"   âš ï¸ {len(target_entries_rel)} æ¡æ ‡æ³¨éƒ½è®¡ç®—ä¸ºæ— æ•ˆä½ç½®ï¼")

                            if cfg["entries"]["convert"] and target_entries_rel:
                                file_fmt = cfg["entries"]["output_format"]
                                entry_out_path = os.path.join(output_folder, study_id, f"{datalog_id}.{file_fmt}")
                                
                                entryplanter = EntryPlanter(target_entries_rel)
                                filter_groups = cfg["entries"]["filter_annotation_type"]
                                criteria = {"groups": filter_groups} if filter_groups else {}
                                
                                if file_fmt == "csv":
                                    entryplanter.savecsv(entry_out_path, criteria=criteria, ref_timestamp=0)
                                elif file_fmt == "sel":
                                    entryplanter.savesel(entry_out_path, 0, fs, list(mappings.keys()), criteria=criteria)
                                
                                conv_logger.info(f"   -> ğŸ“„ ç²¾ç¡®ç”Ÿæˆ: {datalog_id}.{file_fmt} ({len(target_entries_rel)}æ¡)")

                        except Exception as e:
                            conv_logger.error(f"âŒ æ–‡ä»¶ {datalog_id} è½¬æ¢å¤±è´¥: {str(e)}")
                            continue
                        
            conv_logger.info(f"âœ… å…¨éƒ¨å®Œæˆ! å…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
            res_logs = mem_handler.logs
            conv_logger.removeHandler(mem_handler)
            return True, res_logs
        
    except Exception as e:
        import traceback
        err = traceback.format_exc()
        conv_logger.error(f"âŒ ç³»ç»Ÿé”™è¯¯:\n{err}")
        res_logs = mem_handler.logs
        conv_logger.removeHandler(mem_handler)
        return False, res_logs

@app.route('/')
def home():
    """ è®¿é—®ä¸»é¡µä¸­å¿ƒ """
    html_path = resource_path('ui/index.html')
    if not os.path.exists(html_path):
        return f"UI é¦–é¡µç¼ºå¤±ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {html_path}", 404
        
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return render_template_string(content)
    except Exception as e:
        return f"æ— æ³•åŠ è½½é¦–é¡µ: {e}", 500

@app.route('/ui/<path:filename>')
def serve_ui(filename):
    """
    ç»Ÿä¸€å¤„ç† /ui/ è·¯å¾„ä¸‹çš„é™æ€èµ„äº§ã€‚
    åŒ…æ‹¬ HTMLï¼ˆè‡ªåŠ¨æ³¨å…¥å¯¼èˆªï¼‰ã€JSã€CSS å’Œå›¾åƒã€‚
    """
    import re
    from flask import make_response, send_from_directory
    
    ui_base = resource_path('ui')
    file_full_path = os.path.join(ui_base, filename)
    
    if not os.path.exists(file_full_path):
        return f"èµ„äº§æœªæ‰¾åˆ°: {filename}", 404
        
    # å¤„ç†é HTML é™æ€èµ„æº (tailwind.js, vue.js ç­‰)
    if not filename.lower().endswith('.html'):
        return send_from_directory(ui_base, filename)
        
    # å¤„ç†å­é¡µ HTML (è‡ªåŠ¨æ³¨å…¥è¿”å›ä¸»ä¸­å¿ƒçš„æŒ‰é’®)
    try:
        with open(file_full_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}", 500

    # ä»…å‘é index.html çš„ HTML æ–‡ä»¶æ³¨å…¥è¿”å›å¯¼èˆª
    if 'index.html' not in filename.lower():
        nav_injection = """
        <div id="epycon-home-nav" style="position:fixed; top:12px; right:12px; z-index:9999; opacity:0.9;">
            <a href="/" style="background:#0f172a; color:white; padding:8px 16px; border-radius:8px; text-decoration:none; font-size:13px; font-family:sans-serif; font-weight:500; box-shadow:0 4px 6px -1px rgba(0,0,0,0.1), 0 2px 4px -1px rgba(0,0,0,0.06); border:1px solid #334155;">
                â† è¿”å›æ•°æ®ä¸­å¿ƒ
            </a>
        </div>
        """
        # åœ¨ <body> æ ‡ç­¾åæ³¨å…¥
        body_match = re.search(r'<\s*body[^>]*>', content, re.IGNORECASE | re.DOTALL)
        if body_match:
            end_pos = body_match.end()
            content = content[:end_pos] + nav_injection + content[end_pos:]
        else:
            content = nav_injection + content
            
    response = make_response(content)
    response.headers['Content-Type'] = 'text/html; charset=utf-8'
    return response

@app.route('/vendor/<path:filename>')
def serve_vendor_compatibility(filename):
    """ 
    å…¼å®¹é€»è¾‘ï¼šå…è®¸æ ¹è·¯å¾„ä¸‹çš„ index.html é€šè¿‡ç›¸å¯¹è·¯å¾„ 'vendor/...' è®¿é—®èµ„æºã€‚
    è¿™ä½¿å¾— HTML åœ¨ç›´æ¥åŒå‡»æ‰“å¼€å’Œé€šè¿‡ Flask è®¿é—®æ—¶éƒ½èƒ½æ‰¾åˆ° CSS/JSã€‚
    """
    return send_from_directory(resource_path('ui/vendor'), filename)

@app.route('/<filename>.html')
def serve_html_compatibility(filename):
    """
    å…¼å®¹é€»è¾‘ï¼šå…è®¸æ ¹è·¯å¾„ä¸‹çš„è¯·æ±‚é‡å®šå‘åˆ° /ui/ è·¯å¾„ã€‚
    ä¾‹å¦‚è¯·æ±‚ /editor.html ä¼šæ˜ å°„åˆ° serve_ui('editor.html')
    """
    return serve_ui(f"{filename}.html")

@app.route('/run-direct', methods=['POST'])
def run_direct():
    with open("flask_route_called.txt", "w", encoding="utf-8") as f:
        f.write("âœ… /run-direct route was called!\n")
    with open("flask_debug.txt", "a", encoding="utf-8") as f:
        f.write("\n>>> /run-direct CALLED\n")
        f.flush()
    try:
        config_data = request.json
        with open("flask_debug.txt", "a", encoding="utf-8") as f:
            f.write(f"config_data.get('paths')={config_data.get('paths', {})}\n")
            f.flush()
        success, logs = execute_epycon_conversion(config_data)
        with open("flask_debug.txt", "a", encoding="utf-8") as f:
            f.write(f"execute_epycon_conversion returned success={success}\n")
            f.flush()
        return jsonify({"status": "success" if success else "error", "logs": "\n".join(logs)})
    except Exception as e:
        import traceback
        error_msg = f"Flask endpoint error: {str(e)}\n{traceback.format_exc()}"
        with open("flask_debug.txt", "a", encoding="utf-8") as f:
            f.write(f"Exception: {error_msg}\n")
            f.flush()
        return jsonify({"status": "error", "logs": error_msg}), 500

@app.route('/api/select-folder', methods=['GET'])
def api_select_folder():
    try:
        root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
        path = filedialog.askdirectory(); root.destroy()
        if path: path = os.path.normpath(path)
        return jsonify({"path": path})
    except Exception as e:
        return jsonify({"error": str(e), "path": ""})

@app.route('/api/shutdown', methods=['POST'])
def api_shutdown():
    """
    å…³é—­ Epycon GUI çš„ API ç«¯ç‚¹
    """
    try:
        response = jsonify({"status": "shutting_down", "message": "ç¨‹åºæ­£åœ¨å…³é—­..."})
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå…³é—­
        def shutdown_worker():
            time.sleep(0.5)  # ç­‰å¾… HTTP å“åº”å‘é€å®Œæ¯•
            cleanup_on_exit()
            import os as os_module
            os_module._exit(0)
        
        shutdown_thread = threading.Thread(target=shutdown_worker, daemon=True)
        shutdown_thread.start()
        
        return response
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/api/restart', methods=['POST'])
def api_restart():
    """
    é‡å¯ Flask æœåŠ¡çš„ API ç«¯ç‚¹ã€‚
    è¿”å›æˆåŠŸåï¼Œå‰ç«¯ä¼šç­‰å¾…2ç§’å†åˆ·æ–°ã€‚
    """
    try:
        import subprocess
        import sys
        
        # ç«‹å³è¿”å›æˆåŠŸå“åº”
        response = jsonify({"status": "restarting", "message": "æœåŠ¡æ­£åœ¨é‡å¯ï¼Œè¯·ç¨å€™..."})
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œé‡å¯ï¼ˆä¸é˜»å¡å½“å‰è¯·æ±‚ï¼‰
        def restart_worker():
            import time
            time.sleep(1)  # ç­‰å¾… HTTP å“åº”å‘é€å®Œæ¯•
            
            # åœ¨åå°å¯åŠ¨æ–°çš„ app_gui.py è¿›ç¨‹
            subprocess.Popen([sys.executable, 'app_gui.py'], cwd=os.getcwd())
            
            # ç­‰å¾…æ–°è¿›ç¨‹å¯åŠ¨åï¼Œå…³é—­å½“å‰è¿›ç¨‹
            time.sleep(2)
            import os as os_module
            os_module._exit(0)
        
        restart_thread = threading.Thread(target=restart_worker, daemon=True)
        restart_thread.start()
        
        return response
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def open_browser(port=5000):
    try:
        url = f"http://127.0.0.1:{port}/"
        logging.getLogger(__name__).info(f"Opening browser to {url}")
        if os.environ.get('NO_BROWSER') != '1':
            webbrowser.open(url)
        else:
            print(f"è·³è¿‡æ‰“å¼€æµè§ˆå™¨ (NO_BROWSER=1)ï¼Œè¯·æ‰‹åŠ¨è®¿é—®: {url}")
    except Exception as e:
        logging.getLogger(__name__).error(f"Failed to open browser: {e}")
        print(f"è¯·æ‰‹åŠ¨æ‰“å¼€æµè§ˆå™¨è®¿é—®: {url}")

if __name__ == '__main__':
    try:
        # ç¡®ä¿å·¥ä½œç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        os.chdir(script_dir)
        
        # è¯†åˆ«æ˜¯å¦ä¸ºæ‰“åŒ…åçš„ EXE
        is_frozen = getattr(sys, 'frozen', False)
        
        for stream in (sys.stdout, sys.stderr):
            # Use a concrete type check so static analyzers (Pylance) know this
            # object supports `reconfigure`. `io.TextIOWrapper` exposes
            # reconfigure() on Python 3.7+.
            if isinstance(stream, io.TextIOWrapper):
                try:
                    stream.reconfigure(encoding="utf-8", errors="replace")
                except Exception:
                    pass
        
        # 1. ç«¯å£ç®¡ç†
        # ä½¿ç”¨ç¯å¢ƒå˜é‡åŒæ­¥çˆ¶å­è¿›ç¨‹ç«¯å£ï¼Œé˜²æ­¢å­è¿›ç¨‹äºŒæ¬¡æ¢æµ‹æ—¶å‘ç”Ÿæ¼‚ç§»
        env_port = os.environ.get('EPYCON_ACTUAL_PORT')
        if env_port:
            port = int(env_port)
        else:
            preferred_port = 5000
            port = preferred_port
            if not check_port_available(port):
                # æš´åŠ›æ¸…ç†å ç”¨è€…ï¼ˆå¦‚æ—§çš„ WorkMateDataCenterï¼‰
                kill_port_occupier(port)
                time.sleep(1.5) # ç»™ç³»ç»Ÿæ›´å¤šæ—¶é—´é‡Šæ”¾ç«¯å£
                
                if not check_port_available(port):
                    print(f"âš ï¸ ç«¯å£ {port} ä»åœ¨ä½¿ç”¨ä¸­ï¼Œå°è¯•æœç´¢å¤‡ç”¨ç«¯å£...")
                    found = False
                    for p in range(port + 1, port + 51):
                        if check_port_available(p):
                            port = p
                            found = True
                            print(f"âœ… é€‰æ‹©å¤‡ç”¨ç«¯å£: {port}")
                            break
                    if not found:
                        print("âŒ æœªæ‰¾åˆ° 5000-5050 èŒƒå›´å†…çš„å¯ç”¨ç«¯å£ä¾›åº”ã€‚")
                        input("\næŒ‰å›è½¦é”®é€€å‡º...")
                        sys.exit(1)
            # å­˜å…¥ç¯å¢ƒå˜é‡ï¼Œä¾›å­è¿›ç¨‹ä½¿ç”¨
            os.environ['EPYCON_ACTUAL_PORT'] = str(port)

        # æ£€æŸ¥æ˜¯å¦æ˜¯ Flask Reloader çš„çˆ¶è¿›ç¨‹
        is_reloader_parent = (not is_frozen and 
                              not os.environ.get('WERKZEUG_RUN_MAIN'))
        
        # å•å®ä¾‹æ£€æŸ¥å¿…é¡»åœ¨ Reloader çˆ¶è¿›ç¨‹ä¸­æ‰§è¡Œï¼ˆé˜²æ­¢å¤šä¸ªå®ä¾‹å¯åŠ¨ï¼‰
        print("ğŸ” æ­£åœ¨è¿›è¡Œå¯åŠ¨å‰æ£€æŸ¥...")
        if not check_single_instance():
            print("\nâŒ ç¨‹åºå·²åœ¨è¿è¡Œï¼Œæ— æ³•å¯åŠ¨æ–°å®ä¾‹ã€‚")
            print("æç¤ºï¼šå¦‚æœç¡®è®¤æ²¡æœ‰å…¶ä»–å®ä¾‹ï¼Œè¯·åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š")
            print(f"      {os.path.join(tempfile.gettempdir(), 'epycon_gui.lock')}")
            input("\næŒ‰å›è½¦é”®é€€å‡º...")
            sys.exit(1)
        
        print("âœ… å¯åŠ¨æ£€æŸ¥é€šè¿‡")

        # å¦‚æœä»¥ PyInstaller æ‰“åŒ…ä¸º EXE å¹¶åœ¨ Windows ä¸Šè¿è¡Œï¼Œæœ€å°åŒ–æ§åˆ¶å°çª—å£
        try:
            if is_frozen and os.name == 'nt':
                import ctypes
                SW_MINIMIZE = 6
                hWnd = ctypes.windll.kernel32.GetConsoleWindow()
                if hWnd:
                    ctypes.windll.user32.ShowWindow(hWnd, SW_MINIMIZE)
        except Exception:
            pass

        print("\nğŸš€ Epycon GUI (V68.3) å¯åŠ¨ä¸­...")
        print("ğŸ“Œ PID:", os.getpid())
        print(f"ğŸŒ è®¿é—®åœ°å€: http://127.0.0.1:{port}/")
        print("ğŸ’¡ æç¤º: å¯åœ¨é¡µé¢ä¸­ç‚¹å‡»'é€€å‡ºç¨‹åº'æŒ‰é’®å…³é—­ï¼Œæˆ–æŒ‰ Ctrl+C é€€å‡º\n")
        
        # æ³¨å†Œä¿¡å·å¤„ç†ï¼ˆä¼˜é›…é€€å‡ºï¼‰
        def signal_handler(sig, frame):
            print("\n\nğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
            cleanup_on_exit()
            print("âœ… æ¸…ç†å®Œæˆï¼Œç¨‹åºå·²é€€å‡º")
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        if hasattr(signal, 'SIGTERM'):
            signal.signal(signal.SIGTERM, signal_handler)
        
        # å¯¹äº EXE ç‰ˆæœ¬ï¼Œç¦ç”¨ reloaderï¼ˆé¿å…è¿›ç¨‹ç®¡ç†é—®é¢˜ï¼‰
        use_reloader = not is_frozen
        
        # ä»…åœ¨å·¥ä½œè¿›ç¨‹ä¸­æ‰“å¼€æµè§ˆå™¨ï¼Œé¿å… reloader å¯¼è‡´æ‰“å¼€ä¸¤æ¬¡
        # WERKZEUG_RUN_MAIN='true' è¡¨ç¤ºè¿™æ˜¯ Flask çš„å®é™…å·¥ä½œè¿›ç¨‹
        if not is_frozen and os.environ.get('WERKZEUG_RUN_MAIN') == 'true':
            # å»¶è¿Ÿæ‰“å¼€æµè§ˆå™¨ï¼Œç¡®ä¿æœåŠ¡å™¨å®Œå…¨å¯åŠ¨
            threading.Thread(
                target=lambda: (time.sleep(2), open_browser(port)),
                daemon=True
            ).start()
        elif is_frozen:
            # EXE ç‰ˆæœ¬ä¸ä½¿ç”¨ reloaderï¼Œç›´æ¥å»¶è¿Ÿæ‰“å¼€
            threading.Thread(
                target=lambda: (time.sleep(2), open_browser(port)),
                daemon=True
            ).start()
            
        # å¯åŠ¨æœåŠ¡å™¨
        # ä½¿ç”¨ host='0.0.0.0' é€šå¸¸èƒ½è§£å†³ Windows ä¸Šçš„ "è¿æ¥è¢«æ‹’ç»" é—®é¢˜
        app.run(
            host='0.0.0.0',
            port=port,
            debug=not is_frozen, 
            use_reloader=False,
            threaded=True
        )
    except Exception as e:
        print(f"å¯åŠ¨é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()